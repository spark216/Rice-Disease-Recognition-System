{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a685b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a37ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Classes: ['Bacterial Leaf Blight', 'Brown Spot', 'Healthy Rice Leaf', 'Leaf Blast', 'Leaf scald', 'Narrow Brown Leaf Spot', 'Neck Blast', 'Rice Hispa', 'Sheath Blight']\n",
      "\n",
      "===== Training efficientnet-b0 model =====\n",
      "\n",
      "efficientnet-b0 Classifier:\n",
      "Sequential(\n",
      "  (0): Dropout(p=0.2, inplace=True)\n",
      "  (1): Linear(in_features=1280, out_features=9, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet-b0 Parameters: 15.33 MB\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 0.9182 Acc: 0.6795 Precision: 0.6755 Recall: 0.6778 F1: 0.6745\n",
      "val Loss: 0.4494 Acc: 0.8413 Precision: 0.8494 Recall: 0.8414 F1: 0.8417\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.5360 Acc: 0.8162 Precision: 0.8152 Recall: 0.8147 F1: 0.8145\n",
      "val Loss: 0.3300 Acc: 0.8871 Precision: 0.8956 Recall: 0.8939 F1: 0.8886\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.4119 Acc: 0.8601 Precision: 0.8597 Recall: 0.8586 F1: 0.8590\n",
      "val Loss: 0.2110 Acc: 0.9198 Precision: 0.9191 Recall: 0.9188 F1: 0.9175\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.3296 Acc: 0.8897 Precision: 0.8899 Recall: 0.8897 F1: 0.8896\n",
      "val Loss: 0.2328 Acc: 0.9193 Precision: 0.9214 Recall: 0.9225 F1: 0.9184\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.3005 Acc: 0.8981 Precision: 0.8994 Recall: 0.8987 F1: 0.8989\n",
      "val Loss: 0.1753 Acc: 0.9435 Precision: 0.9437 Recall: 0.9437 F1: 0.9428\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.2586 Acc: 0.9131 Precision: 0.9133 Recall: 0.9126 F1: 0.9129\n",
      "val Loss: 0.1366 Acc: 0.9526 Precision: 0.9530 Recall: 0.9528 F1: 0.9525\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.2324 Acc: 0.9203 Precision: 0.9207 Recall: 0.9198 F1: 0.9202\n",
      "val Loss: 0.1717 Acc: 0.9447 Precision: 0.9514 Recall: 0.9460 F1: 0.9459\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.2149 Acc: 0.9321 Precision: 0.9322 Recall: 0.9325 F1: 0.9323\n",
      "val Loss: 0.1243 Acc: 0.9593 Precision: 0.9612 Recall: 0.9583 F1: 0.9592\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.1661 Acc: 0.9441 Precision: 0.9460 Recall: 0.9443 F1: 0.9451\n",
      "val Loss: 0.0652 Acc: 0.9757 Precision: 0.9751 Recall: 0.9759 F1: 0.9754\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.1914 Acc: 0.9356 Precision: 0.9354 Recall: 0.9355 F1: 0.9354\n",
      "val Loss: 0.0669 Acc: 0.9785 Precision: 0.9778 Recall: 0.9800 F1: 0.9786\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.1637 Acc: 0.9458 Precision: 0.9463 Recall: 0.9469 F1: 0.9465\n",
      "val Loss: 0.0787 Acc: 0.9740 Precision: 0.9737 Recall: 0.9737 F1: 0.9735\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.1622 Acc: 0.9462 Precision: 0.9461 Recall: 0.9455 F1: 0.9457\n",
      "val Loss: 0.0570 Acc: 0.9791 Precision: 0.9811 Recall: 0.9795 F1: 0.9801\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.1625 Acc: 0.9491 Precision: 0.9491 Recall: 0.9488 F1: 0.9489\n",
      "val Loss: 0.0932 Acc: 0.9740 Precision: 0.9716 Recall: 0.9764 F1: 0.9735\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.1389 Acc: 0.9515 Precision: 0.9515 Recall: 0.9520 F1: 0.9518\n",
      "val Loss: 0.1077 Acc: 0.9627 Precision: 0.9621 Recall: 0.9629 F1: 0.9621\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.1267 Acc: 0.9560 Precision: 0.9567 Recall: 0.9568 F1: 0.9567\n",
      "val Loss: 0.0906 Acc: 0.9678 Precision: 0.9689 Recall: 0.9672 F1: 0.9673\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.1227 Acc: 0.9601 Precision: 0.9605 Recall: 0.9606 F1: 0.9606\n",
      "val Loss: 0.0918 Acc: 0.9768 Precision: 0.9747 Recall: 0.9752 F1: 0.9747\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.1317 Acc: 0.9593 Precision: 0.9601 Recall: 0.9597 F1: 0.9599\n",
      "val Loss: 0.0906 Acc: 0.9689 Precision: 0.9691 Recall: 0.9711 F1: 0.9692\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.1128 Acc: 0.9637 Precision: 0.9646 Recall: 0.9645 F1: 0.9645\n",
      "val Loss: 0.0468 Acc: 0.9859 Precision: 0.9854 Recall: 0.9864 F1: 0.9858\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.0997 Acc: 0.9674 Precision: 0.9679 Recall: 0.9677 F1: 0.9678\n",
      "val Loss: 0.0708 Acc: 0.9842 Precision: 0.9843 Recall: 0.9838 F1: 0.9840\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.1112 Acc: 0.9624 Precision: 0.9628 Recall: 0.9630 F1: 0.9629\n",
      "val Loss: 0.0441 Acc: 0.9836 Precision: 0.9828 Recall: 0.9837 F1: 0.9832\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.1235 Acc: 0.9584 Precision: 0.9588 Recall: 0.9584 F1: 0.9586\n",
      "val Loss: 0.0456 Acc: 0.9870 Precision: 0.9878 Recall: 0.9873 F1: 0.9875\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.1040 Acc: 0.9674 Precision: 0.9675 Recall: 0.9675 F1: 0.9675\n",
      "val Loss: 0.0375 Acc: 0.9893 Precision: 0.9887 Recall: 0.9902 F1: 0.9893\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.0987 Acc: 0.9670 Precision: 0.9675 Recall: 0.9676 F1: 0.9675\n",
      "val Loss: 0.0384 Acc: 0.9864 Precision: 0.9866 Recall: 0.9860 F1: 0.9862\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.0986 Acc: 0.9693 Precision: 0.9691 Recall: 0.9696 F1: 0.9694\n",
      "val Loss: 0.0460 Acc: 0.9853 Precision: 0.9869 Recall: 0.9855 F1: 0.9860\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.1001 Acc: 0.9680 Precision: 0.9693 Recall: 0.9686 F1: 0.9689\n",
      "val Loss: 0.0915 Acc: 0.9752 Precision: 0.9762 Recall: 0.9770 F1: 0.9763\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.0893 Acc: 0.9714 Precision: 0.9723 Recall: 0.9717 F1: 0.9720\n",
      "val Loss: 0.0457 Acc: 0.9864 Precision: 0.9848 Recall: 0.9863 F1: 0.9854\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.0968 Acc: 0.9673 Precision: 0.9676 Recall: 0.9682 F1: 0.9679\n",
      "val Loss: 0.0432 Acc: 0.9876 Precision: 0.9882 Recall: 0.9873 F1: 0.9878\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.0993 Acc: 0.9685 Precision: 0.9685 Recall: 0.9682 F1: 0.9684\n",
      "val Loss: 0.0263 Acc: 0.9898 Precision: 0.9899 Recall: 0.9892 F1: 0.9895\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.0673 Acc: 0.9787 Precision: 0.9788 Recall: 0.9790 F1: 0.9789\n",
      "val Loss: 0.0320 Acc: 0.9870 Precision: 0.9866 Recall: 0.9864 F1: 0.9865\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.0699 Acc: 0.9756 Precision: 0.9754 Recall: 0.9761 F1: 0.9758\n",
      "val Loss: 0.0218 Acc: 0.9904 Precision: 0.9902 Recall: 0.9904 F1: 0.9902\n",
      "\n",
      "Training complete in 41m 52s\n",
      "Best val Acc: 0.990401\n",
      "\n",
      "Evaluating efficientnet-b0 model:\n",
      "\n",
      "efficientnet-b0 Test Metrics:\n",
      "Accuracy: 0.9904\n",
      "Macro Precision: 0.9902\n",
      "Macro Recall: 0.9910\n",
      "Macro F1-Score: 0.9906\n",
      "\n",
      "===== Training resnet50 model =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "resnet50 Classifier:\n",
      "Linear(in_features=2048, out_features=9, bias=True)\n",
      "resnet50 Parameters: 89.75 MB\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.4992 Acc: 0.4642 Precision: 0.4462 Recall: 0.4607 F1: 0.4459\n",
      "val Loss: 1.0697 Acc: 0.6273 Precision: 0.6626 Recall: 0.6198 F1: 0.5963\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.0947 Acc: 0.6173 Precision: 0.6111 Recall: 0.6136 F1: 0.6059\n",
      "val Loss: 0.7891 Acc: 0.7357 Precision: 0.7433 Recall: 0.7397 F1: 0.7366\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9745 Acc: 0.6561 Precision: 0.6496 Recall: 0.6518 F1: 0.6477\n",
      "val Loss: 0.9443 Acc: 0.6595 Precision: 0.6835 Recall: 0.6636 F1: 0.6579\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.8988 Acc: 0.6784 Precision: 0.6724 Recall: 0.6732 F1: 0.6701\n",
      "val Loss: 0.6926 Acc: 0.7623 Precision: 0.7683 Recall: 0.7618 F1: 0.7596\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.7921 Acc: 0.7214 Precision: 0.7187 Recall: 0.7169 F1: 0.7157\n",
      "val Loss: 0.9927 Acc: 0.6827 Precision: 0.7748 Recall: 0.6747 F1: 0.6584\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.7372 Acc: 0.7380 Precision: 0.7357 Recall: 0.7341 F1: 0.7331\n",
      "val Loss: 0.5604 Acc: 0.7984 Precision: 0.8340 Recall: 0.7918 F1: 0.7921\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.6963 Acc: 0.7555 Precision: 0.7542 Recall: 0.7516 F1: 0.7513\n",
      "val Loss: 0.4786 Acc: 0.8255 Precision: 0.8319 Recall: 0.8279 F1: 0.8255\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.6512 Acc: 0.7696 Precision: 0.7660 Recall: 0.7643 F1: 0.7639\n",
      "val Loss: 0.4429 Acc: 0.8391 Precision: 0.8463 Recall: 0.8404 F1: 0.8372\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.5979 Acc: 0.7897 Precision: 0.7882 Recall: 0.7858 F1: 0.7860\n",
      "val Loss: 0.5048 Acc: 0.8199 Precision: 0.8292 Recall: 0.8260 F1: 0.8202\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.5864 Acc: 0.7979 Precision: 0.7967 Recall: 0.7942 F1: 0.7946\n",
      "val Loss: 0.5212 Acc: 0.8267 Precision: 0.8462 Recall: 0.8261 F1: 0.8231\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.5391 Acc: 0.8130 Precision: 0.8119 Recall: 0.8095 F1: 0.8101\n",
      "val Loss: 0.3630 Acc: 0.8696 Precision: 0.8756 Recall: 0.8612 F1: 0.8608\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.5174 Acc: 0.8179 Precision: 0.8167 Recall: 0.8149 F1: 0.8153\n",
      "val Loss: 0.3317 Acc: 0.8758 Precision: 0.8785 Recall: 0.8767 F1: 0.8742\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.4804 Acc: 0.8356 Precision: 0.8337 Recall: 0.8324 F1: 0.8327\n",
      "val Loss: 0.2680 Acc: 0.9091 Precision: 0.9121 Recall: 0.9055 F1: 0.9075\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.4477 Acc: 0.8464 Precision: 0.8452 Recall: 0.8427 F1: 0.8435\n",
      "val Loss: 0.3505 Acc: 0.8826 Precision: 0.8868 Recall: 0.8889 F1: 0.8810\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.4447 Acc: 0.8461 Precision: 0.8457 Recall: 0.8432 F1: 0.8441\n",
      "val Loss: 0.2117 Acc: 0.9379 Precision: 0.9382 Recall: 0.9334 F1: 0.9347\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.4081 Acc: 0.8621 Precision: 0.8604 Recall: 0.8588 F1: 0.8590\n",
      "val Loss: 0.3616 Acc: 0.8961 Precision: 0.9055 Recall: 0.8991 F1: 0.8960\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.3920 Acc: 0.8635 Precision: 0.8625 Recall: 0.8606 F1: 0.8612\n",
      "val Loss: 0.3300 Acc: 0.8910 Precision: 0.8972 Recall: 0.8960 F1: 0.8903\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.3770 Acc: 0.8727 Precision: 0.8714 Recall: 0.8717 F1: 0.8714\n",
      "val Loss: 0.2152 Acc: 0.9181 Precision: 0.9188 Recall: 0.9156 F1: 0.9163\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.3706 Acc: 0.8751 Precision: 0.8753 Recall: 0.8735 F1: 0.8741\n",
      "val Loss: 0.2405 Acc: 0.9176 Precision: 0.9159 Recall: 0.9213 F1: 0.9167\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.3341 Acc: 0.8848 Precision: 0.8845 Recall: 0.8841 F1: 0.8842\n",
      "val Loss: 0.1893 Acc: 0.9385 Precision: 0.9388 Recall: 0.9401 F1: 0.9380\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.3282 Acc: 0.8884 Precision: 0.8889 Recall: 0.8875 F1: 0.8880\n",
      "val Loss: 0.1359 Acc: 0.9520 Precision: 0.9505 Recall: 0.9519 F1: 0.9507\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.3192 Acc: 0.8906 Precision: 0.8900 Recall: 0.8895 F1: 0.8895\n",
      "val Loss: 0.1269 Acc: 0.9497 Precision: 0.9501 Recall: 0.9463 F1: 0.9475\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.2965 Acc: 0.9003 Precision: 0.8999 Recall: 0.8997 F1: 0.8997\n",
      "val Loss: 0.2645 Acc: 0.9181 Precision: 0.9246 Recall: 0.9140 F1: 0.9173\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.2877 Acc: 0.9043 Precision: 0.9052 Recall: 0.9042 F1: 0.9046\n",
      "val Loss: 0.1390 Acc: 0.9537 Precision: 0.9527 Recall: 0.9526 F1: 0.9523\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.2911 Acc: 0.9032 Precision: 0.9040 Recall: 0.9023 F1: 0.9029\n",
      "val Loss: 0.1367 Acc: 0.9526 Precision: 0.9515 Recall: 0.9539 F1: 0.9519\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.2607 Acc: 0.9125 Precision: 0.9119 Recall: 0.9105 F1: 0.9111\n",
      "val Loss: 0.1414 Acc: 0.9531 Precision: 0.9557 Recall: 0.9534 F1: 0.9533\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.2773 Acc: 0.9069 Precision: 0.9072 Recall: 0.9061 F1: 0.9065\n",
      "val Loss: 0.1939 Acc: 0.9339 Precision: 0.9402 Recall: 0.9349 F1: 0.9345\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.2643 Acc: 0.9097 Precision: 0.9104 Recall: 0.9097 F1: 0.9100\n",
      "val Loss: 0.1607 Acc: 0.9452 Precision: 0.9497 Recall: 0.9448 F1: 0.9458\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.2299 Acc: 0.9234 Precision: 0.9236 Recall: 0.9229 F1: 0.9232\n",
      "val Loss: 0.2952 Acc: 0.9029 Precision: 0.9208 Recall: 0.8995 F1: 0.9004\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.2499 Acc: 0.9151 Precision: 0.9151 Recall: 0.9142 F1: 0.9145\n",
      "val Loss: 0.1000 Acc: 0.9639 Precision: 0.9636 Recall: 0.9638 F1: 0.9635\n",
      "\n",
      "Training complete in 41m 36s\n",
      "Best val Acc: 0.963862\n",
      "\n",
      "Evaluating resnet50 model:\n",
      "\n",
      "resnet50 Test Metrics:\n",
      "Accuracy: 0.9661\n",
      "Macro Precision: 0.9655\n",
      "Macro Recall: 0.9677\n",
      "Macro F1-Score: 0.9659\n",
      "\n",
      "===== Training shufflenet_v2 model =====\n",
      "\n",
      "shufflenet_v2 Classifier:\n",
      "Linear(in_features=1024, out_features=9, bias=True)\n",
      "shufflenet_v2 Parameters: 4.82 MB\n",
      "Epoch 0/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1`. You can also use `weights=ShuffleNet_V2_X1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1437 Acc: 0.6117 Precision: 0.6114 Recall: 0.5975 F1: 0.5831\n",
      "val Loss: 0.6938 Acc: 0.7595 Precision: 0.7871 Recall: 0.7529 F1: 0.7494\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.6870 Acc: 0.7574 Precision: 0.7503 Recall: 0.7519 F1: 0.7499\n",
      "val Loss: 0.3995 Acc: 0.8498 Precision: 0.8497 Recall: 0.8429 F1: 0.8427\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.5446 Acc: 0.8087 Precision: 0.8081 Recall: 0.8056 F1: 0.8061\n",
      "val Loss: 0.3277 Acc: 0.8826 Precision: 0.8879 Recall: 0.8757 F1: 0.8783\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.4734 Acc: 0.8364 Precision: 0.8359 Recall: 0.8349 F1: 0.8350\n",
      "val Loss: 0.2961 Acc: 0.9063 Precision: 0.9059 Recall: 0.9069 F1: 0.9054\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.4002 Acc: 0.8644 Precision: 0.8638 Recall: 0.8627 F1: 0.8630\n",
      "val Loss: 0.2863 Acc: 0.8950 Precision: 0.8958 Recall: 0.8948 F1: 0.8941\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.3569 Acc: 0.8780 Precision: 0.8766 Recall: 0.8760 F1: 0.8761\n",
      "val Loss: 0.2881 Acc: 0.9051 Precision: 0.9082 Recall: 0.9049 F1: 0.9032\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.3272 Acc: 0.8899 Precision: 0.8901 Recall: 0.8891 F1: 0.8894\n",
      "val Loss: 0.1812 Acc: 0.9385 Precision: 0.9401 Recall: 0.9383 F1: 0.9374\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.2952 Acc: 0.9014 Precision: 0.9015 Recall: 0.9014 F1: 0.9014\n",
      "val Loss: 0.1521 Acc: 0.9469 Precision: 0.9463 Recall: 0.9443 F1: 0.9442\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.2660 Acc: 0.9145 Precision: 0.9149 Recall: 0.9149 F1: 0.9148\n",
      "val Loss: 0.1704 Acc: 0.9356 Precision: 0.9376 Recall: 0.9328 F1: 0.9345\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.2472 Acc: 0.9161 Precision: 0.9163 Recall: 0.9159 F1: 0.9161\n",
      "val Loss: 0.1443 Acc: 0.9537 Precision: 0.9542 Recall: 0.9552 F1: 0.9534\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.2570 Acc: 0.9150 Precision: 0.9154 Recall: 0.9153 F1: 0.9153\n",
      "val Loss: 0.1526 Acc: 0.9497 Precision: 0.9493 Recall: 0.9514 F1: 0.9488\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.2215 Acc: 0.9236 Precision: 0.9237 Recall: 0.9245 F1: 0.9241\n",
      "val Loss: 0.1245 Acc: 0.9588 Precision: 0.9604 Recall: 0.9566 F1: 0.9571\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.2307 Acc: 0.9236 Precision: 0.9235 Recall: 0.9235 F1: 0.9235\n",
      "val Loss: 0.0764 Acc: 0.9712 Precision: 0.9698 Recall: 0.9719 F1: 0.9706\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.2013 Acc: 0.9293 Precision: 0.9296 Recall: 0.9295 F1: 0.9295\n",
      "val Loss: 0.0839 Acc: 0.9718 Precision: 0.9704 Recall: 0.9732 F1: 0.9716\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.1865 Acc: 0.9361 Precision: 0.9359 Recall: 0.9368 F1: 0.9363\n",
      "val Loss: 0.0562 Acc: 0.9774 Precision: 0.9767 Recall: 0.9774 F1: 0.9769\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.1915 Acc: 0.9356 Precision: 0.9354 Recall: 0.9350 F1: 0.9352\n",
      "val Loss: 0.0833 Acc: 0.9684 Precision: 0.9707 Recall: 0.9684 F1: 0.9692\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.1812 Acc: 0.9409 Precision: 0.9415 Recall: 0.9411 F1: 0.9413\n",
      "val Loss: 0.0695 Acc: 0.9768 Precision: 0.9752 Recall: 0.9780 F1: 0.9763\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.1807 Acc: 0.9393 Precision: 0.9396 Recall: 0.9399 F1: 0.9397\n",
      "val Loss: 0.0606 Acc: 0.9797 Precision: 0.9793 Recall: 0.9795 F1: 0.9793\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.1650 Acc: 0.9465 Precision: 0.9469 Recall: 0.9469 F1: 0.9469\n",
      "val Loss: 0.0618 Acc: 0.9785 Precision: 0.9781 Recall: 0.9786 F1: 0.9783\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.1570 Acc: 0.9488 Precision: 0.9497 Recall: 0.9501 F1: 0.9499\n",
      "val Loss: 0.1018 Acc: 0.9650 Precision: 0.9627 Recall: 0.9669 F1: 0.9640\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.1514 Acc: 0.9458 Precision: 0.9467 Recall: 0.9464 F1: 0.9465\n",
      "val Loss: 0.0671 Acc: 0.9701 Precision: 0.9702 Recall: 0.9709 F1: 0.9703\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.1594 Acc: 0.9476 Precision: 0.9479 Recall: 0.9481 F1: 0.9480\n",
      "val Loss: 0.0355 Acc: 0.9881 Precision: 0.9879 Recall: 0.9885 F1: 0.9881\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.1397 Acc: 0.9555 Precision: 0.9555 Recall: 0.9559 F1: 0.9557\n",
      "val Loss: 0.0641 Acc: 0.9797 Precision: 0.9793 Recall: 0.9807 F1: 0.9798\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.1472 Acc: 0.9517 Precision: 0.9517 Recall: 0.9520 F1: 0.9519\n",
      "val Loss: 0.0627 Acc: 0.9780 Precision: 0.9792 Recall: 0.9756 F1: 0.9772\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.1337 Acc: 0.9553 Precision: 0.9558 Recall: 0.9559 F1: 0.9558\n",
      "val Loss: 0.0302 Acc: 0.9893 Precision: 0.9894 Recall: 0.9890 F1: 0.9892\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.1204 Acc: 0.9596 Precision: 0.9604 Recall: 0.9610 F1: 0.9607\n",
      "val Loss: 0.0424 Acc: 0.9859 Precision: 0.9864 Recall: 0.9867 F1: 0.9865\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.1277 Acc: 0.9578 Precision: 0.9588 Recall: 0.9584 F1: 0.9586\n",
      "val Loss: 0.0595 Acc: 0.9785 Precision: 0.9789 Recall: 0.9781 F1: 0.9784\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.1271 Acc: 0.9579 Precision: 0.9585 Recall: 0.9585 F1: 0.9585\n",
      "val Loss: 0.0614 Acc: 0.9768 Precision: 0.9787 Recall: 0.9766 F1: 0.9774\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.1367 Acc: 0.9544 Precision: 0.9551 Recall: 0.9551 F1: 0.9551\n",
      "val Loss: 0.0495 Acc: 0.9836 Precision: 0.9851 Recall: 0.9840 F1: 0.9845\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.1216 Acc: 0.9601 Precision: 0.9606 Recall: 0.9600 F1: 0.9603\n",
      "val Loss: 0.0435 Acc: 0.9876 Precision: 0.9890 Recall: 0.9862 F1: 0.9875\n",
      "\n",
      "Training complete in 40m 52s\n",
      "Best val Acc: 0.989272\n",
      "\n",
      "Evaluating shufflenet_v2 model:\n",
      "\n",
      "shufflenet_v2 Test Metrics:\n",
      "Accuracy: 0.9825\n",
      "Macro Precision: 0.9823\n",
      "Macro Recall: 0.9825\n",
      "Macro F1-Score: 0.9822\n",
      "\n",
      "===== Training mobilenet_v3 model =====\n",
      "\n",
      "mobilenet_v3 Classifier:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=1280, out_features=9, bias=True)\n",
      ")\n",
      "mobilenet_v3 Parameters: 16.07 MB\n",
      "Epoch 0/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9253 Acc: 0.6716 Precision: 0.6663 Recall: 0.6681 F1: 0.6653\n",
      "val Loss: 0.7242 Acc: 0.7708 Precision: 0.7867 Recall: 0.7763 F1: 0.7767\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.6078 Acc: 0.7925 Precision: 0.7909 Recall: 0.7902 F1: 0.7901\n",
      "val Loss: 0.4118 Acc: 0.8532 Precision: 0.8581 Recall: 0.8585 F1: 0.8530\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.4731 Acc: 0.8409 Precision: 0.8407 Recall: 0.8400 F1: 0.8402\n",
      "val Loss: 0.2722 Acc: 0.9068 Precision: 0.9098 Recall: 0.9026 F1: 0.9040\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.3682 Acc: 0.8734 Precision: 0.8740 Recall: 0.8725 F1: 0.8731\n",
      "val Loss: 0.3018 Acc: 0.9119 Precision: 0.9237 Recall: 0.9079 F1: 0.9115\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.3176 Acc: 0.8939 Precision: 0.8934 Recall: 0.8933 F1: 0.8933\n",
      "val Loss: 0.2964 Acc: 0.9001 Precision: 0.9063 Recall: 0.8953 F1: 0.8973\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.2858 Acc: 0.9025 Precision: 0.9029 Recall: 0.9023 F1: 0.9025\n",
      "val Loss: 0.2770 Acc: 0.9006 Precision: 0.9083 Recall: 0.9042 F1: 0.9012\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.2387 Acc: 0.9182 Precision: 0.9186 Recall: 0.9184 F1: 0.9185\n",
      "val Loss: 0.2145 Acc: 0.9283 Precision: 0.9321 Recall: 0.9272 F1: 0.9265\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.2224 Acc: 0.9246 Precision: 0.9251 Recall: 0.9252 F1: 0.9251\n",
      "val Loss: 0.1571 Acc: 0.9481 Precision: 0.9493 Recall: 0.9504 F1: 0.9491\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.1995 Acc: 0.9337 Precision: 0.9347 Recall: 0.9341 F1: 0.9343\n",
      "val Loss: 0.2942 Acc: 0.9198 Precision: 0.9354 Recall: 0.9076 F1: 0.9140\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.2105 Acc: 0.9293 Precision: 0.9303 Recall: 0.9297 F1: 0.9300\n",
      "val Loss: 0.1215 Acc: 0.9582 Precision: 0.9586 Recall: 0.9587 F1: 0.9581\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.1819 Acc: 0.9373 Precision: 0.9374 Recall: 0.9378 F1: 0.9376\n",
      "val Loss: 0.1255 Acc: 0.9571 Precision: 0.9591 Recall: 0.9591 F1: 0.9577\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.1696 Acc: 0.9431 Precision: 0.9436 Recall: 0.9433 F1: 0.9434\n",
      "val Loss: 0.1850 Acc: 0.9424 Precision: 0.9428 Recall: 0.9443 F1: 0.9422\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.1627 Acc: 0.9469 Precision: 0.9472 Recall: 0.9466 F1: 0.9469\n",
      "val Loss: 0.1224 Acc: 0.9582 Precision: 0.9562 Recall: 0.9593 F1: 0.9573\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.1519 Acc: 0.9514 Precision: 0.9517 Recall: 0.9518 F1: 0.9518\n",
      "val Loss: 0.1316 Acc: 0.9537 Precision: 0.9596 Recall: 0.9520 F1: 0.9541\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.1383 Acc: 0.9547 Precision: 0.9554 Recall: 0.9552 F1: 0.9553\n",
      "val Loss: 0.1604 Acc: 0.9509 Precision: 0.9506 Recall: 0.9520 F1: 0.9505\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.1391 Acc: 0.9543 Precision: 0.9546 Recall: 0.9545 F1: 0.9545\n",
      "val Loss: 0.0832 Acc: 0.9746 Precision: 0.9763 Recall: 0.9709 F1: 0.9732\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.1361 Acc: 0.9567 Precision: 0.9569 Recall: 0.9572 F1: 0.9571\n",
      "val Loss: 0.0750 Acc: 0.9729 Precision: 0.9765 Recall: 0.9709 F1: 0.9734\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.1178 Acc: 0.9616 Precision: 0.9623 Recall: 0.9620 F1: 0.9622\n",
      "val Loss: 0.0549 Acc: 0.9808 Precision: 0.9809 Recall: 0.9807 F1: 0.9807\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.1161 Acc: 0.9630 Precision: 0.9632 Recall: 0.9629 F1: 0.9631\n",
      "val Loss: 0.0660 Acc: 0.9791 Precision: 0.9809 Recall: 0.9772 F1: 0.9786\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.1234 Acc: 0.9631 Precision: 0.9638 Recall: 0.9633 F1: 0.9635\n",
      "val Loss: 0.3214 Acc: 0.9232 Precision: 0.9267 Recall: 0.9217 F1: 0.9216\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.1406 Acc: 0.9563 Precision: 0.9560 Recall: 0.9565 F1: 0.9562\n",
      "val Loss: 0.1175 Acc: 0.9627 Precision: 0.9711 Recall: 0.9615 F1: 0.9652\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.1067 Acc: 0.9635 Precision: 0.9646 Recall: 0.9643 F1: 0.9644\n",
      "val Loss: 0.5959 Acc: 0.8549 Precision: 0.8652 Recall: 0.8592 F1: 0.8545\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.1009 Acc: 0.9686 Precision: 0.9687 Recall: 0.9686 F1: 0.9687\n",
      "val Loss: 0.6203 Acc: 0.9221 Precision: 0.9396 Recall: 0.9229 F1: 0.9268\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.1146 Acc: 0.9617 Precision: 0.9621 Recall: 0.9619 F1: 0.9620\n",
      "val Loss: 0.1376 Acc: 0.9673 Precision: 0.9690 Recall: 0.9665 F1: 0.9671\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.1107 Acc: 0.9645 Precision: 0.9650 Recall: 0.9645 F1: 0.9647\n",
      "val Loss: 0.0418 Acc: 0.9853 Precision: 0.9840 Recall: 0.9855 F1: 0.9847\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.1089 Acc: 0.9657 Precision: 0.9659 Recall: 0.9653 F1: 0.9656\n",
      "val Loss: 0.0929 Acc: 0.9757 Precision: 0.9753 Recall: 0.9784 F1: 0.9764\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.1110 Acc: 0.9650 Precision: 0.9654 Recall: 0.9648 F1: 0.9650\n",
      "val Loss: 0.1054 Acc: 0.9684 Precision: 0.9693 Recall: 0.9704 F1: 0.9696\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.1047 Acc: 0.9673 Precision: 0.9681 Recall: 0.9678 F1: 0.9680\n",
      "val Loss: 0.0883 Acc: 0.9774 Precision: 0.9766 Recall: 0.9788 F1: 0.9774\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.0869 Acc: 0.9733 Precision: 0.9737 Recall: 0.9735 F1: 0.9736\n",
      "val Loss: 0.0833 Acc: 0.9791 Precision: 0.9799 Recall: 0.9775 F1: 0.9785\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.0960 Acc: 0.9710 Precision: 0.9713 Recall: 0.9713 F1: 0.9713\n",
      "val Loss: 0.0960 Acc: 0.9774 Precision: 0.9800 Recall: 0.9769 F1: 0.9780\n",
      "\n",
      "Training complete in 40m 59s\n",
      "Best val Acc: 0.985319\n",
      "\n",
      "Evaluating mobilenet_v3 model:\n",
      "\n",
      "mobilenet_v3 Test Metrics:\n",
      "Accuracy: 0.9853\n",
      "Macro Precision: 0.9843\n",
      "Macro Recall: 0.9866\n",
      "Macro F1-Score: 0.9853\n",
      "\n",
      "===== Model Comparison =====\n",
      "Model           Accuracy(%)  Precision(%) Recall(%)    F1-Score(%)  Params(MB)  \n",
      "---------------------------------------------------------------------------\n",
      "efficientnet-b0 99.04        99.02        99.10        99.06        15.33       \n",
      "resnet50        96.61        96.55        96.77        96.59        89.75       \n",
      "shufflenet_v2   98.25        98.23        98.25        98.22        4.82        \n",
      "mobilenet_v3    98.53        98.43        98.66        98.53        16.07       \n",
      "\n",
      "===== Training and evaluation completed for all models =====\n",
      "Training curves saved to 'training_curves_comparison.png'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# 设置随机种子确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 配置参数\n",
    "DATA_DIR = r\"D:\\homework\\论文\\论文\\project\\code\\训练\\dataset_split\"\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# 模型配置（替换VGG16为ShuffleNet v2）\n",
    "MODELS = {\n",
    "    \"efficientnet-b0\": {\n",
    "        \"model\": models.efficientnet_b0,\n",
    "        \"pretrained\": True,\n",
    "        \"feature_size\": 1280,\n",
    "        \"classifier\": lambda feat_size, num_classes: nn.Linear(feat_size, num_classes)\n",
    "    },\n",
    "    \"resnet50\": {\n",
    "        \"model\": models.resnet50,\n",
    "        \"pretrained\": True,\n",
    "        \"feature_size\": 2048,\n",
    "        \"classifier\": lambda feat_size, num_classes: nn.Linear(feat_size, num_classes)\n",
    "    },\n",
    "    \"shufflenet_v2\": {\n",
    "        \"model\": models.shufflenet_v2_x1_0,\n",
    "        \"pretrained\": True,\n",
    "        \"feature_size\": 1024,\n",
    "        \"classifier\": lambda feat_size, num_classes: nn.Linear(feat_size, num_classes)\n",
    "    },\n",
    "    \"mobilenet_v3\": {\n",
    "        \"model\": models.mobilenet_v3_large,\n",
    "        \"pretrained\": True,\n",
    "        \"feature_size\": 960,  # 修正特征维度\n",
    "        \"classifier\": lambda feat_size, num_classes: nn.Sequential(\n",
    "            nn.Linear(feat_size, 1280),\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(1280, num_classes)\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "# 检查GPU是否可用\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def create_data_transforms():\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(p=0.1),\n",
    "            transforms.RandomRotation(30),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.3,\n",
    "                contrast=0.3,\n",
    "                saturation=0.3,\n",
    "                hue=0.1\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            # transforms.RandomErasing(p=0.2, scale=(0.02, 0.2)),\n",
    "            transforms.RandomAffine(\n",
    "                degrees=0,\n",
    "                translate=(0.1, 0.1),\n",
    "                scale=(0.9, 1.1),\n",
    "                shear=10\n",
    "            ),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.CenterCrop(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.CenterCrop(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms\n",
    "\n",
    "def create_data_loaders(data_dir):\n",
    "    \"\"\"创建数据加载器\"\"\"\n",
    "    data_transforms = create_data_transforms()\n",
    "    \n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                      for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    dataloaders = {x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True if x == 'train' else False,\n",
    "                                 num_workers=NUM_WORKERS)\n",
    "                   for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    \n",
    "    return dataloaders, dataset_sizes, class_names\n",
    "\n",
    "def initialize_model(model_name, num_classes):\n",
    "    \"\"\"初始化预训练模型\"\"\"\n",
    "    model_info = MODELS[model_name]\n",
    "    model = model_info[\"model\"](pretrained=model_info[\"pretrained\"])\n",
    "    \n",
    "    # 修改分类器\n",
    "    if model_name == \"resnet50\":\n",
    "        model.fc = model_info[\"classifier\"](model_info[\"feature_size\"], num_classes)\n",
    "    elif model_name == \"shufflenet_v2\":\n",
    "        model.fc = model_info[\"classifier\"](model_info[\"feature_size\"], num_classes)\n",
    "    elif model_name == \"efficientnet-b0\":\n",
    "        model.classifier[1] = model_info[\"classifier\"](model_info[\"feature_size\"], num_classes)\n",
    "    elif model_name == \"mobilenet_v3\":\n",
    "        model.classifier = model_info[\"classifier\"](model_info[\"feature_size\"], num_classes)\n",
    "    \n",
    "    # 打印模型结构（可选）\n",
    "    print(f\"\\n{model_name} Classifier:\")\n",
    "    print(model.classifier if hasattr(model, 'classifier') else model.fc)\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"计算模型参数量（MB）\"\"\"\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return params * 4 / (1024 * 1024)  # 转换为MB\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, model_name=\"model\"):\n",
    "    \"\"\"训练模型并记录指标\"\"\"\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # 记录训练历史\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_precision': [], 'train_recall': [], 'train_f1': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_precision': [], 'val_recall': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # 每个epoch都有一个训练和验证阶段\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 训练模式\n",
    "            else:\n",
    "                model.eval()   # 评估模式\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            # 迭代数据\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # 零梯度\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向传播\n",
    "                # 只有在训练时才跟踪历史\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # 只有在训练阶段才进行反向传播和优化\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # 统计\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # 计算epoch指标\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            # 计算分类报告\n",
    "            report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
    "            macro_precision = report['macro avg']['precision']\n",
    "            macro_recall = report['macro avg']['recall']\n",
    "            macro_f1 = report['macro avg']['f1-score']\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} '\n",
    "                  f'Precision: {macro_precision:.4f} Recall: {macro_recall:.4f} F1: {macro_f1:.4f}')\n",
    "            \n",
    "            # 记录历史\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
    "            history[f'{phase}_precision'].append(macro_precision)\n",
    "            history[f'{phase}_recall'].append(macro_recall)\n",
    "            history[f'{phase}_f1'].append(macro_f1)\n",
    "            \n",
    "            # 深拷贝模型\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), f\"{model_name}_best_model.pth\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    \n",
    "    # 加载最佳模型权重\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names, model_name):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 打印分类报告\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    print(f\"\\n{model_name} Test Metrics:\")\n",
    "    print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
    "    print(f\"Macro Precision: {report['macro avg']['precision']:.4f}\")\n",
    "    print(f\"Macro Recall: {report['macro avg']['recall']:.4f}\")\n",
    "    print(f\"Macro F1-Score: {report['macro avg']['f1-score']:.4f}\")\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'precision': report['macro avg']['precision'],\n",
    "        'recall': report['macro avg']['recall'],\n",
    "        'f1': report['macro avg']['f1-score'],\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def plot_training_curves(histories, metrics, save_path):\n",
    "    \"\"\"绘制训练曲线\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        \n",
    "        for model_name, history in histories.items():\n",
    "            plt.plot(history[f'train_{metric}'], label=f'{model_name} Train')\n",
    "            plt.plot(history[f'val_{metric}'], label=f'{model_name} Val')\n",
    "        \n",
    "        plt.title(f'Model {metric.capitalize()}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def compare_models_table(model_metrics, model_sizes):\n",
    "    \"\"\"生成模型对比表格\"\"\"\n",
    "    print(\"\\n===== Model Comparison =====\")\n",
    "    print(f\"{'Model':<15} {'Accuracy(%)':<12} {'Precision(%)':<12} {'Recall(%)':<12} {'F1-Score(%)':<12} {'Params(MB)':<12}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for model_name, metrics in model_metrics.items():\n",
    "        print(f\"{model_name:<15} {metrics['accuracy']*100:<12.2f} {metrics['precision']*100:<12.2f} \"\n",
    "              f\"{metrics['recall']*100:<12.2f} {metrics['f1']*100:<12.2f} {model_sizes[model_name]:<12.2f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 创建数据加载器\n",
    "    dataloaders, dataset_sizes, class_names = create_data_loaders(DATA_DIR)\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    all_histories = {}\n",
    "    all_metrics = {}\n",
    "    model_sizes = {}\n",
    "    \n",
    "    # 训练所有模型\n",
    "    for model_name in MODELS.keys():\n",
    "        print(f\"\\n===== Training {model_name} model =====\")\n",
    "        \n",
    "        # 初始化模型\n",
    "        model = initialize_model(model_name, num_classes)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # 计算模型大小\n",
    "        model_size = count_parameters(model)\n",
    "        model_sizes[model_name] = model_size\n",
    "        print(f\"{model_name} Parameters: {model_size:.2f} MB\")\n",
    "        \n",
    "        # 设置损失函数和优化器\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # 训练模型\n",
    "        model, history = train_model(\n",
    "            model, dataloaders, criterion, optimizer,\n",
    "            num_epochs=EPOCHS, model_name=model_name\n",
    "        )\n",
    "        \n",
    "        # 评估模型\n",
    "        print(f\"\\nEvaluating {model_name} model:\")\n",
    "        metrics = evaluate_model(model, dataloaders['test'], class_names, model_name)\n",
    "        all_metrics[model_name] = metrics\n",
    "        \n",
    "        # 保存训练历史\n",
    "        all_histories[model_name] = history\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    plot_training_curves(\n",
    "        all_histories, \n",
    "        metrics=['loss', 'acc', 'precision', 'recall', 'f1'],\n",
    "        save_path='training_curves_comparison.png'\n",
    "    )\n",
    "    \n",
    "    # 生成模型对比表格\n",
    "    compare_models_table(all_metrics, model_sizes)\n",
    "    \n",
    "    print(\"\\n===== Training and evaluation completed for all models =====\")\n",
    "    print(\"Training curves saved to 'training_curves_comparison.png'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2cc71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "GPU: NVIDIA GeForce RTX 3060 Ti\n",
      "Processing class: Bacterial Leaf Blight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 837/837 [00:05<00:00, 149.62it/s]\n",
      "Copying val files: 100%|██████████| 180/180 [00:01<00:00, 149.75it/s]\n",
      "Copying test files: 100%|██████████| 180/180 [00:01<00:00, 154.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Brown Spot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 1082/1082 [00:06<00:00, 154.82it/s]\n",
      "Copying val files: 100%|██████████| 232/232 [00:01<00:00, 158.13it/s]\n",
      "Copying test files: 100%|██████████| 232/232 [00:01<00:00, 161.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Healthy Rice Leaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 759/759 [00:05<00:00, 151.58it/s]\n",
      "Copying val files: 100%|██████████| 163/163 [00:01<00:00, 155.35it/s]\n",
      "Copying test files: 100%|██████████| 163/163 [00:01<00:00, 149.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Leaf Blast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 1222/1222 [00:07<00:00, 154.88it/s]\n",
      "Copying val files: 100%|██████████| 263/263 [00:01<00:00, 152.45it/s]\n",
      "Copying test files: 100%|██████████| 263/263 [00:01<00:00, 152.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Leaf scald\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 932/932 [00:05<00:00, 156.28it/s]\n",
      "Copying val files: 100%|██████████| 200/200 [00:01<00:00, 156.51it/s]\n",
      "Copying test files: 100%|██████████| 200/200 [00:01<00:00, 155.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Narrow Brown Leaf Spot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 667/667 [00:04<00:00, 155.44it/s]\n",
      "Copying val files: 100%|██████████| 143/143 [00:00<00:00, 160.08it/s]\n",
      "Copying test files: 100%|██████████| 144/144 [00:00<00:00, 151.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Neck Blast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 700/700 [00:05<00:00, 122.61it/s]\n",
      "Copying val files: 100%|██████████| 150/150 [00:01<00:00, 120.61it/s]\n",
      "Copying test files: 100%|██████████| 150/150 [00:01<00:00, 120.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Rice Hispa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 909/909 [00:05<00:00, 157.99it/s]\n",
      "Copying val files: 100%|██████████| 195/195 [00:01<00:00, 161.39it/s]\n",
      "Copying test files: 100%|██████████| 195/195 [00:01<00:00, 159.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Sheath Blight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 1139/1139 [00:07<00:00, 154.80it/s]\n",
      "Copying val files: 100%|██████████| 245/245 [00:01<00:00, 153.83it/s]\n",
      "Copying test files: 100%|██████████| 245/245 [00:01<00:00, 158.84it/s]\n",
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed. Saved to dataset_split\n",
      "Classes: ['Bacterial Leaf Blight', 'Brown Spot', 'Healthy Rice Leaf', 'Leaf Blast', 'Leaf scald', 'Narrow Brown Leaf Spot', 'Neck Blast', 'Rice Hispa', 'Sheath Blight']\n",
      "\n",
      "===== Training efficientnet-b0 model =====\n",
      "Epoch 0/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9142 Acc: 0.6846: 100%|██████████| 258/258 [01:05<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9142 Acc: 0.6846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5424 Acc: 0.8204: 100%|██████████| 56/56 [00:09<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5424 Acc: 0.8204\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5424 Acc: 0.8152: 100%|██████████| 258/258 [00:55<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5424 Acc: 0.8152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3305 Acc: 0.8871: 100%|██████████| 56/56 [00:08<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3305 Acc: 0.8871\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4165 Acc: 0.8598: 100%|██████████| 258/258 [00:55<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4165 Acc: 0.8598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3152 Acc: 0.9130: 100%|██████████| 56/56 [00:08<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3152 Acc: 0.9130\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3429 Acc: 0.8836: 100%|██████████| 258/258 [00:54<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3429 Acc: 0.8836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1845 Acc: 0.9356: 100%|██████████| 56/56 [00:08<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1845 Acc: 0.9356\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3020 Acc: 0.8952: 100%|██████████| 258/258 [00:53<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3020 Acc: 0.8952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1369 Acc: 0.9571: 100%|██████████| 56/56 [00:08<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1369 Acc: 0.9571\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2434 Acc: 0.9205: 100%|██████████| 258/258 [00:54<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2434 Acc: 0.9205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1997 Acc: 0.9413: 100%|██████████| 56/56 [00:08<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1997 Acc: 0.9413\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2220 Acc: 0.9228: 100%|██████████| 258/258 [00:55<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2220 Acc: 0.9228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1019 Acc: 0.9667: 100%|██████████| 56/56 [00:08<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1019 Acc: 0.9667\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1326 Acc: 0.9537: 100%|██████████| 258/258 [00:55<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1326 Acc: 0.9537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0449 Acc: 0.9848: 100%|██████████| 56/56 [00:08<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0449 Acc: 0.9848\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0893 Acc: 0.9720: 100%|██████████| 258/258 [00:54<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0893 Acc: 0.9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0273 Acc: 0.9910: 100%|██████████| 56/56 [00:08<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0273 Acc: 0.9910\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0810 Acc: 0.9742: 100%|██████████| 258/258 [00:54<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0810 Acc: 0.9742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0252 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0252 Acc: 0.9932\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0683 Acc: 0.9777: 100%|██████████| 258/258 [00:54<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0683 Acc: 0.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0206 Acc: 0.9910: 100%|██████████| 56/56 [00:08<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0206 Acc: 0.9910\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0698 Acc: 0.9762: 100%|██████████| 258/258 [00:53<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0698 Acc: 0.9762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0147 Acc: 0.9955: 100%|██████████| 56/56 [00:08<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0147 Acc: 0.9955\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0533 Acc: 0.9823: 100%|██████████| 258/258 [00:54<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0533 Acc: 0.9823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0138 Acc: 0.9966: 100%|██████████| 56/56 [00:08<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0138 Acc: 0.9966\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0536 Acc: 0.9845: 100%|██████████| 258/258 [00:54<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0536 Acc: 0.9845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0122 Acc: 0.9955: 100%|██████████| 56/56 [00:08<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0122 Acc: 0.9955\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0479 Acc: 0.9859: 100%|██████████| 258/258 [00:53<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0479 Acc: 0.9859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0122 Acc: 0.9972: 100%|██████████| 56/56 [00:08<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0122 Acc: 0.9972\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0447 Acc: 0.9853: 100%|██████████| 258/258 [00:54<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0447 Acc: 0.9853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0126 Acc: 0.9955: 100%|██████████| 56/56 [00:08<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0126 Acc: 0.9955\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0438 Acc: 0.9851: 100%|██████████| 258/258 [00:53<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0438 Acc: 0.9851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0102 Acc: 0.9966: 100%|██████████| 56/56 [00:08<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0102 Acc: 0.9966\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0416 Acc: 0.9858: 100%|██████████| 258/258 [00:53<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0416 Acc: 0.9858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0103 Acc: 0.9960: 100%|██████████| 56/56 [00:08<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0103 Acc: 0.9960\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0411 Acc: 0.9871: 100%|██████████| 258/258 [00:53<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0411 Acc: 0.9871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0101 Acc: 0.9972: 100%|██████████| 56/56 [00:08<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0101 Acc: 0.9972\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0477 Acc: 0.9851: 100%|██████████| 258/258 [00:53<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0477 Acc: 0.9851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0106 Acc: 0.9960: 100%|██████████| 56/56 [00:08<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0106 Acc: 0.9960\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0414 Acc: 0.9867: 100%|██████████| 258/258 [00:54<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0414 Acc: 0.9867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0099 Acc: 0.9972: 100%|██████████| 56/56 [00:08<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0099 Acc: 0.9972\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0429 Acc: 0.9853: 100%|██████████| 258/258 [00:53<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0429 Acc: 0.9853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0088 Acc: 0.9977: 100%|██████████| 56/56 [00:08<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0088 Acc: 0.9977\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0423 Acc: 0.9870: 100%|██████████| 258/258 [00:54<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0423 Acc: 0.9870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0096 Acc: 0.9977: 100%|██████████| 56/56 [00:08<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0096 Acc: 0.9977\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0399 Acc: 0.9880: 100%|██████████| 258/258 [00:53<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0399 Acc: 0.9880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0090 Acc: 0.9977: 100%|██████████| 56/56 [00:08<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0090 Acc: 0.9977\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0378 Acc: 0.9869: 100%|██████████| 258/258 [00:53<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0378 Acc: 0.9869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0092 Acc: 0.9972: 100%|██████████| 56/56 [00:07<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0092 Acc: 0.9972\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0378 Acc: 0.9880: 100%|██████████| 258/258 [00:53<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0378 Acc: 0.9880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0090 Acc: 0.9977: 100%|██████████| 56/56 [00:07<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0090 Acc: 0.9977\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0360 Acc: 0.9887: 100%|██████████| 258/258 [00:53<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0360 Acc: 0.9887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0088 Acc: 0.9983: 100%|██████████| 56/56 [00:08<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0088 Acc: 0.9983\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0411 Acc: 0.9878: 100%|██████████| 258/258 [00:53<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0411 Acc: 0.9878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0092 Acc: 0.9972: 100%|██████████| 56/56 [00:07<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0092 Acc: 0.9972\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0388 Acc: 0.9873: 100%|██████████| 258/258 [00:53<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0388 Acc: 0.9873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0090 Acc: 0.9972: 100%|██████████| 56/56 [00:07<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0090 Acc: 0.9972\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0400 Acc: 0.9863: 100%|██████████| 258/258 [00:53<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0400 Acc: 0.9863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0092 Acc: 0.9977: 100%|██████████| 56/56 [00:08<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0092 Acc: 0.9977\n",
      "\n",
      "Training complete in 42m 13s\n",
      "Best val Acc: 0.998306\n",
      "\n",
      "Evaluating efficientnet-b0 model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 56/56 [00:20<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      " Bacterial Leaf Blight       0.99      0.98      0.99       180\n",
      "            Brown Spot       1.00      0.99      1.00       232\n",
      "     Healthy Rice Leaf       1.00      1.00      1.00       163\n",
      "            Leaf Blast       0.98      1.00      0.99       263\n",
      "            Leaf scald       1.00      0.98      0.99       200\n",
      "Narrow Brown Leaf Spot       0.99      1.00      0.99       144\n",
      "            Neck Blast       1.00      1.00      1.00       150\n",
      "            Rice Hispa       1.00      1.00      1.00       195\n",
      "         Sheath Blight       1.00      1.00      1.00       245\n",
      "\n",
      "              accuracy                           0.99      1772\n",
      "             macro avg       0.99      0.99      0.99      1772\n",
      "          weighted avg       0.99      0.99      0.99      1772\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "===== Training completed for all models =====\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.efficientnet import MBConv\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置随机种子确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 配置参数\n",
    "DATA_DIR = r\"D:\\homework\\论文\\论文\\project\\dataset\\archive\\Rice_Leaf_AUG\\Rice_Leaf_AUG\"  # 原始数据集路径\n",
    "OUTPUT_DIR = \"dataset_split\"  # 划分后的数据集保存路径\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "NUM_WORKERS = 4  # 数据加载的线程数\n",
    "SPLIT_RATIO = [0.7, 0.15, 0.15]  # 训练集、验证集、测试集比例\n",
    "\n",
    "# 模型配置\n",
    "MODELS = {\n",
    "    \"efficientnet-b0\": {\n",
    "        \"model\": models.efficientnet_b0,\n",
    "        \"pretrained\": True,\n",
    "        \"feature_size\": 1280  # EfficientNet-B0的特征维度\n",
    "    },\n",
    "}\n",
    "\n",
    "# 检查GPU是否可用\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 注意力模块定义\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction_ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // reduction_ratio, channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return x * self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(out)\n",
    "        return x * self.sigmoid(out)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction_ratio=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(channels, reduction_ratio)\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.channel_attention(x)\n",
    "        x = self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "def add_cbam_to_efficientnet(model):\n",
    "    \"\"\"为EfficientNet添加CBAM模块\"\"\"\n",
    "    features = []\n",
    "    for layer in model.features.children():\n",
    "        features.append(layer)\n",
    "        if isinstance(layer, MBConv):\n",
    "            # 获取当前MBConv层的输出通道数\n",
    "            out_channels = layer.out_channels\n",
    "            # 添加CBAM模块\n",
    "            features.append(CBAM(out_channels))\n",
    "    # 重构features模块\n",
    "    model.features = nn.Sequential(*features)\n",
    "    return model\n",
    "\n",
    "def split_dataset(data_dir, output_dir, split_ratio):\n",
    "    \"\"\"将数据集划分为训练集、验证集和测试集\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    subdirs = [f.name for f in os.scandir(data_dir) if f.is_dir()]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        print(f\"Processing class: {subdir}\")\n",
    "        class_dir = os.path.join(data_dir, subdir)\n",
    "        files = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "        \n",
    "        # 划分数据集\n",
    "        train_files, test_files = train_test_split(files, test_size=split_ratio[2], random_state=42)\n",
    "        train_files, val_files = train_test_split(train_files, test_size=split_ratio[1]/(split_ratio[0]+split_ratio[1]), random_state=42)\n",
    "        \n",
    "        # 创建输出目录\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            split_dir = os.path.join(output_dir, split, subdir)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        # 复制文件\n",
    "        def copy_files(files_list, split_name):\n",
    "            for file_name in tqdm(files_list, desc=f\"Copying {split_name} files\"):\n",
    "                src = os.path.join(class_dir, file_name)\n",
    "                dst = os.path.join(output_dir, split_name, subdir, file_name)\n",
    "                shutil.copy(src, dst)\n",
    "        \n",
    "        copy_files(train_files, \"train\")\n",
    "        copy_files(val_files, \"val\")\n",
    "        copy_files(test_files, \"test\")\n",
    "    \n",
    "    print(f\"Dataset split completed. Saved to {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "def create_data_transforms():\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(p=0.1),\n",
    "            transforms.RandomRotation(30),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.3,\n",
    "                contrast=0.3,\n",
    "                saturation=0.3,\n",
    "                hue=0.1\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            transforms.RandomAffine(\n",
    "                degrees=0,\n",
    "                translate=(0.1, 0.1),\n",
    "                scale=(0.9, 1.1),\n",
    "                shear=10\n",
    "            ),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.CenterCrop(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.CenterCrop(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms\n",
    "\n",
    "def create_data_loaders(data_dir):\n",
    "    \"\"\"创建数据加载器\"\"\"\n",
    "    data_transforms = create_data_transforms()\n",
    "    \n",
    "    # 创建数据集\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    dataloaders = {x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True if x == 'train' else False,\n",
    "                                 num_workers=NUM_WORKERS)\n",
    "                   for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    \n",
    "    return dataloaders, dataset_sizes, class_names\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract=True):\n",
    "    \"\"\"初始化预训练模型\"\"\"\n",
    "    model_info = MODELS[model_name]\n",
    "    model_ft = model_info[\"model\"](pretrained=model_info[\"pretrained\"])\n",
    "    \n",
    "    # 添加注意力模块（仅对efficientnet）\n",
    "    if model_name == \"efficientnet-b0\":\n",
    "        model_ft = add_cbam_to_efficientnet(model_ft)\n",
    "    \n",
    "    # 冻结预训练模型的参数\n",
    "    if feature_extract:\n",
    "        for param in model_ft.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # 修改分类器\n",
    "    if model_name == \"efficientnet-b0\":\n",
    "        in_features = model_ft.classifier[1].in_features\n",
    "        model_ft.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    return model_ft\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, model_name=\"model\"):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # 记录训练历史\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # 每个epoch都有一个训练和验证阶段\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # 迭代数据\n",
    "            progress_bar = tqdm(enumerate(dataloaders[phase]), total=len(dataloaders[phase]))\n",
    "            for i, (inputs, labels) in progress_bar:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # 零梯度\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向传播\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # 反向传播+优化\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # 统计\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                # 更新进度条\n",
    "                progress_bar.set_description(f\"{phase} Loss: {running_loss/(i*BATCH_SIZE+inputs.size(0)):.4f} Acc: {running_corrects/(i*BATCH_SIZE+inputs.size(0)):.4f}\")\n",
    "            \n",
    "            if phase == 'train' and scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # 记录历史\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
    "            \n",
    "            # 保存最佳模型\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), f\"{model_name}_best_model.pth\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    \n",
    "    # 加载最佳模型权重\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names, model_name):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 打印分类报告+\n",
    "    print(f\"\\n{classification_report(all_labels, all_preds, target_names=class_names)}\")\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plot_confusion_matrix(cm, class_names, f\"{model_name}_confusion_matrix.png\")\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, save_path, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"绘制混淆矩阵\"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_training_curves(histories, save_path='training_curves.png'):\n",
    "    \"\"\"绘制训练曲线\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # 绘制准确率曲线\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for model_name, history in histories.items():\n",
    "        plt.plot(history['train_acc'], label=f'{model_name} Train')\n",
    "        plt.plot(history['val_acc'], label=f'{model_name} Val')\n",
    "    \n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for model_name, history in histories.items():\n",
    "        plt.plot(history['train_loss'], label=f'{model_name} Train')\n",
    "        plt.plot(history['val_loss'], label=f'{model_name} Val')\n",
    "    \n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 划分数据集\n",
    "    split_dir = split_dataset(DATA_DIR, OUTPUT_DIR, SPLIT_RATIO)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    dataloaders, dataset_sizes, class_names = create_data_loaders(split_dir)\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    all_histories = {}\n",
    "    \n",
    "    # 训练所有模型\n",
    "    for model_name in MODELS.keys():\n",
    "        print(f\"\\n===== Training {model_name} model =====\")\n",
    "        \n",
    "        # 初始化模型\n",
    "        model_ft = initialize_model(model_name, num_classes, feature_extract=False)\n",
    "        model_ft = model_ft.to(device)\n",
    "        \n",
    "        # 设置损失函数和优化器\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "        exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "        \n",
    "        # 训练模型\n",
    "        model_ft, history = train_model(\n",
    "            model_ft, dataloaders, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "            num_epochs=EPOCHS, model_name=model_name\n",
    "        )\n",
    "        \n",
    "        # 保存最终模型\n",
    "        torch.save(model_ft.state_dict(), f\"{model_name}_final_model.pth\")\n",
    "        \n",
    "        # 评估模型\n",
    "        print(f\"\\nEvaluating {model_name} model:\")\n",
    "        evaluate_model(model_ft, dataloaders['test'], class_names, model_name)\n",
    "        \n",
    "        # 保存训练历史\n",
    "        all_histories[model_name] = history\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    plot_training_curves(all_histories, save_path='all_models_training_curves.png')\n",
    "    \n",
    "    print(\"\\n===== Training completed for all models =====\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb2bc36",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f677bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Classes: ['Bacterial Leaf Blight', 'Brown Spot', 'Healthy Rice Leaf', 'Leaf Blast', 'Leaf scald', 'Narrow Brown Leaf Spot', 'Neck Blast', 'Rice Hispa', 'Sheath Blight']\n",
      "\n",
      "===== Training efficientnet-b0 model =====\n",
      "Freezing layers for efficientnet-b0...\n",
      "Trainable params: 3,672,017 (91.36%)\n",
      "efficientnet-b0 Trainable Parameters: 14.01 MB\n",
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 367\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining curves saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_curves_comparison.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 367\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 340\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    337\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mrequires_grad, model\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m--> 340\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 202\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloaders, criterion, optimizer, num_epochs, model_name)\u001b[0m\n\u001b[0;32m    199\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# 统计\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    203\u001b[0m running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m    205\u001b[0m all_preds\u001b[38;5;241m.\u001b[39mextend(preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# 设置随机种子确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 配置参数\n",
    "DATA_DIR = r\"D:\\homework\\论文\\论文\\project\\code\\训练\\dataset_split\"\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "NUM_WORKERS = 4\n",
    "FREEZE_LAYERS = True  # 是否冻结部分层\n",
    "\n",
    "# 模型配置\n",
    "MODELS = {\n",
    "        \"efficientnet-b0\": {\n",
    "        \"model\": models.efficientnet_b0,\n",
    "        \"pretrained\": True,\n",
    "        \"feature_size\": 1280,\n",
    "        \"classifier\": lambda feat_size, num_classes: nn.Linear(feat_size, num_classes),\n",
    "        \"freeze_func\": lambda model: [\n",
    "            # 冻结特征提取器的前半部分\n",
    "            param.requires_grad_(False) for param in list(model.features.parameters())[:len(list(model.features.parameters()))//2]\n",
    "        ]\n",
    "    },\n",
    "    \"resnet50\": {\n",
    "        \"model\": models.resnet50,\n",
    "        \"pretrained\": True,\n",
    "        \"feature_size\": 2048,\n",
    "        \"classifier\": lambda feat_size, num_classes: nn.Linear(feat_size, num_classes),\n",
    "        \"freeze_func\": lambda model: [\n",
    "            # 冻结前3个残差块（共4个）\n",
    "            param.requires_grad_(False) for param in list(model.parameters())[:len(list(model.parameters()))//2]\n",
    "        ]\n",
    "    },\n",
    "    \"shufflenet_v2\": {\n",
    "        \"model\": models.shufflenet_v2_x1_0,\n",
    "        \"pretrained\": True,\n",
    "        \"feature_size\": 1024,\n",
    "        \"classifier\": lambda feat_size, num_classes: nn.Linear(feat_size, num_classes),\n",
    "        \"freeze_func\": lambda model: [\n",
    "            # 冻结前半部分层\n",
    "            param.requires_grad_(False) for param in list(model.parameters())[:len(list(model.parameters()))//2]\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    \"mobilenet_v3\": {\n",
    "        \"model\": models.mobilenet_v3_large,\n",
    "        \"pretrained\": True,\n",
    "        \"feature_size\": 960,\n",
    "        \"classifier\": lambda feat_size, num_classes: nn.Sequential(\n",
    "            nn.Linear(feat_size, 1280),\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(1280, num_classes)\n",
    "        ),\n",
    "        \"freeze_func\": lambda model: [\n",
    "            # 冻结特征提取器的前半部分\n",
    "            param.requires_grad_(False) for param in list(model.features.parameters())[:len(list(model.features.parameters()))//2]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 检查GPU是否可用\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def create_data_transforms():\n",
    "    \"\"\"创建数据转换\"\"\"\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(IMAGE_SIZE),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.CenterCrop(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.CenterCrop(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms\n",
    "\n",
    "def create_data_loaders(data_dir):\n",
    "    \"\"\"创建数据加载器\"\"\"\n",
    "    data_transforms = create_data_transforms()\n",
    "    \n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                      for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    dataloaders = {x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True if x == 'train' else False,\n",
    "                                 num_workers=NUM_WORKERS)\n",
    "                   for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    \n",
    "    return dataloaders, dataset_sizes, class_names\n",
    "\n",
    "def initialize_model(model_name, num_classes):\n",
    "    \"\"\"初始化预训练模型并冻结部分层\"\"\"\n",
    "    model_info = MODELS[model_name]\n",
    "    model = model_info[\"model\"](pretrained=model_info[\"pretrained\"])\n",
    "    \n",
    "    # 修改分类器\n",
    "    if model_name == \"resnet50\":\n",
    "        model.fc = model_info[\"classifier\"](model_info[\"feature_size\"], num_classes)\n",
    "    elif model_name == \"shufflenet_v2\":\n",
    "        model.fc = model_info[\"classifier\"](model_info[\"feature_size\"], num_classes)\n",
    "    elif model_name == \"efficientnet-b0\":\n",
    "        model.classifier[1] = model_info[\"classifier\"](model_info[\"feature_size\"], num_classes)\n",
    "    elif model_name == \"mobilenet_v3\":\n",
    "        model.classifier = model_info[\"classifier\"](model_info[\"feature_size\"], num_classes)\n",
    "    \n",
    "    # 冻结部分层\n",
    "    if FREEZE_LAYERS:\n",
    "        print(f\"Freezing layers for {model_name}...\")\n",
    "        model_info[\"freeze_func\"](model)\n",
    "        \n",
    "        # 计算可训练参数\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"计算模型参数量（MB）\"\"\"\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return params * 4 / (1024 * 1024)  # 转换为MB\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, model_name=\"model\"):\n",
    "    \"\"\"训练模型并记录指标\"\"\"\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # 记录训练历史\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_precision': [], 'train_recall': [], 'train_f1': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_precision': [], 'val_recall': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # 每个epoch都有一个训练和验证阶段\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 训练模式\n",
    "            else:\n",
    "                model.eval()   # 评估模式\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            # 迭代数据\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # 零梯度\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向传播\n",
    "                # 只有在训练时才跟踪历史\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # 只有在训练阶段才进行反向传播和优化\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # 统计\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # 计算epoch指标\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            # 计算分类报告\n",
    "            report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
    "            macro_precision = report['macro avg']['precision']\n",
    "            macro_recall = report['macro avg']['recall']\n",
    "            macro_f1 = report['macro avg']['f1-score']\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} '\n",
    "                  f'Precision: {macro_precision:.4f} Recall: {macro_recall:.4f} F1: {macro_f1:.4f}')\n",
    "            \n",
    "            # 记录历史\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
    "            history[f'{phase}_precision'].append(macro_precision)\n",
    "            history[f'{phase}_recall'].append(macro_recall)\n",
    "            history[f'{phase}_f1'].append(macro_f1)\n",
    "            \n",
    "            # 深拷贝模型\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), f\"{model_name}_best_model.pth\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    \n",
    "    # 加载最佳模型权重\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names, model_name):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 打印分类报告\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    print(f\"\\n{model_name} Test Metrics:\")\n",
    "    print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
    "    print(f\"Macro Precision: {report['macro avg']['precision']:.4f}\")\n",
    "    print(f\"Macro Recall: {report['macro avg']['recall']:.4f}\")\n",
    "    print(f\"Macro F1-Score: {report['macro avg']['f1-score']:.4f}\")\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'precision': report['macro avg']['precision'],\n",
    "        'recall': report['macro avg']['recall'],\n",
    "        'f1': report['macro avg']['f1-score'],\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def plot_training_curves(histories, metrics, save_path):\n",
    "    \"\"\"绘制训练曲线\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        \n",
    "        for model_name, history in histories.items():\n",
    "            plt.plot(history[f'train_{metric}'], label=f'{model_name} Train')\n",
    "            plt.plot(history[f'val_{metric}'], label=f'{model_name} Val')\n",
    "        \n",
    "        plt.title(f'Model {metric.capitalize()}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def compare_models_table(model_metrics, model_sizes):\n",
    "    \"\"\"生成模型对比表格\"\"\"\n",
    "    print(\"\\n===== Model Comparison =====\")\n",
    "    print(f\"{'Model':<15} {'Accuracy(%)':<12} {'Precision(%)':<12} {'Recall(%)':<12} {'F1-Score(%)':<12} {'Params(MB)':<12}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for model_name, metrics in model_metrics.items():\n",
    "        print(f\"{model_name:<15} {metrics['accuracy']*100:<12.2f} {metrics['precision']*100:<12.2f} \"\n",
    "              f\"{metrics['recall']*100:<12.2f} {metrics['f1']*100:<12.2f} {model_sizes[model_name]:<12.2f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 创建数据加载器\n",
    "    dataloaders, dataset_sizes, class_names = create_data_loaders(DATA_DIR)\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    all_histories = {}\n",
    "    all_metrics = {}\n",
    "    model_sizes = {}\n",
    "    \n",
    "    # 训练所有模型\n",
    "    for model_name in MODELS.keys():\n",
    "        print(f\"\\n===== Training {model_name} model =====\")\n",
    "        \n",
    "        # 初始化模型\n",
    "        model = initialize_model(model_name, num_classes)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # 计算模型大小\n",
    "        model_size = count_parameters(model)\n",
    "        model_sizes[model_name] = model_size\n",
    "        print(f\"{model_name} Trainable Parameters: {model_size:.2f} MB\")\n",
    "        \n",
    "        # 设置损失函数和优化器\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.1)\n",
    "        \n",
    "        # 训练模型\n",
    "        model, history = train_model(\n",
    "            model, dataloaders, criterion, optimizer,\n",
    "            num_epochs=EPOCHS, model_name=model_name\n",
    "        )\n",
    "        \n",
    "        # 评估模型\n",
    "        print(f\"\\nEvaluating {model_name} model:\")\n",
    "        metrics = evaluate_model(model, dataloaders['test'], class_names, model_name)\n",
    "        all_metrics[model_name] = metrics\n",
    "        \n",
    "        # 保存训练历史\n",
    "        all_histories[model_name] = history\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    plot_training_curves(\n",
    "        all_histories, \n",
    "        metrics=['loss', 'acc', 'precision', 'recall', 'f1'],\n",
    "        save_path='training_curves_comparison.png'\n",
    "    )\n",
    "    \n",
    "    # 生成模型对比表格\n",
    "    compare_models_table(all_metrics, model_sizes)\n",
    "    \n",
    "    print(\"\\n===== Training and evaluation completed for all models =====\")\n",
    "    print(\"Training curves saved to 'training_curves_comparison.png'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d3a9be",
   "metadata": {},
   "source": [
    "最终优化 冻结层 CA注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f3d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "GPU: NVIDIA GeForce RTX 3060 Ti\n",
      "Processing class: Bacterial Leaf Blight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 837/837 [00:00<00:00, 1087.08it/s]\n",
      "Copying val files: 100%|██████████| 180/180 [00:00<00:00, 1098.26it/s]\n",
      "Copying test files: 100%|██████████| 180/180 [00:00<00:00, 1173.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Brown Spot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 1082/1082 [00:00<00:00, 1082.01it/s]\n",
      "Copying val files: 100%|██████████| 232/232 [00:00<00:00, 1212.35it/s]\n",
      "Copying test files: 100%|██████████| 232/232 [00:00<00:00, 1216.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Healthy Rice Leaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 759/759 [00:00<00:00, 994.88it/s] \n",
      "Copying val files: 100%|██████████| 163/163 [00:00<00:00, 1116.52it/s]\n",
      "Copying test files: 100%|██████████| 163/163 [00:00<00:00, 941.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Leaf Blast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 1222/1222 [00:01<00:00, 1088.27it/s]\n",
      "Copying val files: 100%|██████████| 263/263 [00:00<00:00, 1187.17it/s]\n",
      "Copying test files: 100%|██████████| 263/263 [00:00<00:00, 1126.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Leaf scald\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 932/932 [00:00<00:00, 1067.68it/s]\n",
      "Copying val files: 100%|██████████| 200/200 [00:00<00:00, 987.32it/s]\n",
      "Copying test files: 100%|██████████| 200/200 [00:00<00:00, 1240.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Narrow Brown Leaf Spot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 667/667 [00:00<00:00, 1017.94it/s]\n",
      "Copying val files: 100%|██████████| 143/143 [00:00<00:00, 1076.84it/s]\n",
      "Copying test files: 100%|██████████| 144/144 [00:00<00:00, 1081.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Neck Blast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 700/700 [00:01<00:00, 509.07it/s]\n",
      "Copying val files: 100%|██████████| 150/150 [00:00<00:00, 484.44it/s]\n",
      "Copying test files: 100%|██████████| 150/150 [00:00<00:00, 500.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Rice Hispa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 909/909 [00:00<00:00, 1133.50it/s]\n",
      "Copying val files: 100%|██████████| 195/195 [00:00<00:00, 1237.68it/s]\n",
      "Copying test files: 100%|██████████| 195/195 [00:00<00:00, 1106.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Sheath Blight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 1139/1139 [00:01<00:00, 992.93it/s] \n",
      "Copying val files: 100%|██████████| 245/245 [00:00<00:00, 1015.88it/s]\n",
      "Copying test files: 100%|██████████| 245/245 [00:00<00:00, 1199.09it/s]\n",
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\config\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed. Saved to dataset_split\n",
      "Classes: ['Bacterial Leaf Blight', 'Brown Spot', 'Healthy Rice Leaf', 'Leaf Blast', 'Leaf scald', 'Narrow Brown Leaf Spot', 'Neck Blast', 'Rice Hispa', 'Sheath Blight']\n",
      "\n",
      "===== Training efficientnet-b0 model =====\n",
      "Epoch 0/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9132 Acc: 0.6849: 100%|██████████| 258/258 [01:09<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9132 Acc: 0.6849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5736 Acc: 0.8063: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5736 Acc: 0.8063\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5376 Acc: 0.8139: 100%|██████████| 258/258 [00:54<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5376 Acc: 0.8139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3479 Acc: 0.8842: 100%|██████████| 56/56 [00:08<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3479 Acc: 0.8842\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4160 Acc: 0.8598: 100%|██████████| 258/258 [00:55<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4160 Acc: 0.8598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2462 Acc: 0.9142: 100%|██████████| 56/56 [00:08<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2462 Acc: 0.9142\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3439 Acc: 0.8829: 100%|██████████| 258/258 [00:54<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3439 Acc: 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2100 Acc: 0.9294: 100%|██████████| 56/56 [00:08<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2100 Acc: 0.9294\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3037 Acc: 0.8975: 100%|██████████| 258/258 [00:53<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3037 Acc: 0.8975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1786 Acc: 0.9424: 100%|██████████| 56/56 [00:08<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1786 Acc: 0.9424\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2497 Acc: 0.9156: 100%|██████████| 258/258 [00:54<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2497 Acc: 0.9156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1235 Acc: 0.9588: 100%|██████████| 56/56 [00:08<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1235 Acc: 0.9588\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2426 Acc: 0.9162: 100%|██████████| 258/258 [00:55<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2426 Acc: 0.9162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1111 Acc: 0.9616: 100%|██████████| 56/56 [00:08<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1111 Acc: 0.9616\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1382 Acc: 0.9548: 100%|██████████| 258/258 [00:55<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1382 Acc: 0.9548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0494 Acc: 0.9819: 100%|██████████| 56/56 [00:08<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0494 Acc: 0.9819\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0909 Acc: 0.9694: 100%|██████████| 258/258 [00:55<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0909 Acc: 0.9694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0425 Acc: 0.9853: 100%|██████████| 56/56 [00:08<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0425 Acc: 0.9853\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0810 Acc: 0.9741: 100%|██████████| 258/258 [00:55<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0810 Acc: 0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0376 Acc: 0.9887: 100%|██████████| 56/56 [00:08<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0376 Acc: 0.9887\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0698 Acc: 0.9774: 100%|██████████| 258/258 [00:55<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0698 Acc: 0.9774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0264 Acc: 0.9904: 100%|██████████| 56/56 [00:08<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0264 Acc: 0.9904\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0625 Acc: 0.9796: 100%|██████████| 258/258 [00:54<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0625 Acc: 0.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0270 Acc: 0.9910: 100%|██████████| 56/56 [00:08<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0270 Acc: 0.9910\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0564 Acc: 0.9808: 100%|██████████| 258/258 [00:55<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0564 Acc: 0.9808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0243 Acc: 0.9927: 100%|██████████| 56/56 [00:08<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0243 Acc: 0.9927\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0520 Acc: 0.9829: 100%|██████████| 258/258 [00:54<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0520 Acc: 0.9829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0219 Acc: 0.9910: 100%|██████████| 56/56 [00:08<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0219 Acc: 0.9910\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0488 Acc: 0.9838: 100%|██████████| 258/258 [00:54<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0488 Acc: 0.9838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0235 Acc: 0.9904: 100%|██████████| 56/56 [00:08<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0235 Acc: 0.9904\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0436 Acc: 0.9850: 100%|██████████| 258/258 [00:55<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0436 Acc: 0.9850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0207 Acc: 0.9921: 100%|██████████| 56/56 [00:08<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0207 Acc: 0.9921\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0444 Acc: 0.9871: 100%|██████████| 258/258 [00:54<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0444 Acc: 0.9871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0182 Acc: 0.9927: 100%|██████████| 56/56 [00:08<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0182 Acc: 0.9927\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0466 Acc: 0.9842: 100%|██████████| 258/258 [00:54<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0466 Acc: 0.9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0173 Acc: 0.9921: 100%|██████████| 56/56 [00:08<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0173 Acc: 0.9921\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0390 Acc: 0.9886: 100%|██████████| 258/258 [00:53<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0390 Acc: 0.9886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0169 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0169 Acc: 0.9932\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0463 Acc: 0.9846: 100%|██████████| 258/258 [00:54<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0463 Acc: 0.9846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0186 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0186 Acc: 0.9932\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0411 Acc: 0.9858: 100%|██████████| 258/258 [00:54<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0411 Acc: 0.9858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0172 Acc: 0.9921: 100%|██████████| 56/56 [00:08<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0172 Acc: 0.9921\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0432 Acc: 0.9853: 100%|██████████| 258/258 [00:54<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0432 Acc: 0.9853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0159 Acc: 0.9927: 100%|██████████| 56/56 [00:08<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0159 Acc: 0.9927\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0394 Acc: 0.9871: 100%|██████████| 258/258 [00:55<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0394 Acc: 0.9871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0166 Acc: 0.9938: 100%|██████████| 56/56 [00:08<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0166 Acc: 0.9938\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0389 Acc: 0.9876: 100%|██████████| 258/258 [00:54<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0389 Acc: 0.9876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0165 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0165 Acc: 0.9932\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0405 Acc: 0.9871: 100%|██████████| 258/258 [00:54<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0405 Acc: 0.9871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0171 Acc: 0.9927: 100%|██████████| 56/56 [00:08<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0171 Acc: 0.9927\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0389 Acc: 0.9875: 100%|██████████| 258/258 [00:53<00:00,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0389 Acc: 0.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.0170 Acc: 0.9927: 100%|██████████| 56/56 [00:08<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0170 Acc: 0.9927\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0402 Acc: 0.9859: 100%|██████████| 258/258 [00:54<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0402 Acc: 0.9859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0177 Acc: 0.9921: 100%|██████████| 56/56 [00:08<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0177 Acc: 0.9921\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0396 Acc: 0.9874: 100%|██████████| 258/258 [00:53<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0396 Acc: 0.9874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0168 Acc: 0.9921: 100%|██████████| 56/56 [00:08<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0168 Acc: 0.9921\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0405 Acc: 0.9879: 100%|██████████| 258/258 [00:54<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0405 Acc: 0.9879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0157 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0157 Acc: 0.9932\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0364 Acc: 0.9874: 100%|██████████| 258/258 [00:53<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0364 Acc: 0.9874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0160 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0160 Acc: 0.9932\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0443 Acc: 0.9861: 100%|██████████| 258/258 [00:54<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0443 Acc: 0.9861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0157 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0157 Acc: 0.9932\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0430 Acc: 0.9861: 100%|██████████| 258/258 [00:53<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0430 Acc: 0.9861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0150 Acc: 0.9944: 100%|██████████| 56/56 [00:08<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0150 Acc: 0.9944\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0406 Acc: 0.9865: 100%|██████████| 258/258 [00:53<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0406 Acc: 0.9865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0161 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0161 Acc: 0.9932\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0394 Acc: 0.9867: 100%|██████████| 258/258 [00:53<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0394 Acc: 0.9867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0167 Acc: 0.9927: 100%|██████████| 56/56 [00:08<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0167 Acc: 0.9927\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0364 Acc: 0.9893: 100%|██████████| 258/258 [00:53<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0364 Acc: 0.9893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0150 Acc: 0.9938: 100%|██████████| 56/56 [00:08<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0150 Acc: 0.9938\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0461 Acc: 0.9863: 100%|██████████| 258/258 [00:53<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0461 Acc: 0.9863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0158 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0158 Acc: 0.9932\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0430 Acc: 0.9884: 100%|██████████| 258/258 [00:53<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0430 Acc: 0.9884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0160 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0160 Acc: 0.9932\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0411 Acc: 0.9857: 100%|██████████| 258/258 [00:54<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0411 Acc: 0.9857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0167 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0167 Acc: 0.9932\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0391 Acc: 0.9876: 100%|██████████| 258/258 [00:54<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0391 Acc: 0.9876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0147 Acc: 0.9938: 100%|██████████| 56/56 [00:08<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0147 Acc: 0.9938\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0403 Acc: 0.9865: 100%|██████████| 258/258 [00:53<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0403 Acc: 0.9865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0176 Acc: 0.9921: 100%|██████████| 56/56 [00:08<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0176 Acc: 0.9921\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0390 Acc: 0.9870: 100%|██████████| 258/258 [00:53<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0390 Acc: 0.9870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0171 Acc: 0.9938: 100%|██████████| 56/56 [00:08<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0171 Acc: 0.9938\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0381 Acc: 0.9873: 100%|██████████| 258/258 [00:53<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0381 Acc: 0.9873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0150 Acc: 0.9944: 100%|██████████| 56/56 [00:08<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0150 Acc: 0.9944\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0416 Acc: 0.9869: 100%|██████████| 258/258 [00:54<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0416 Acc: 0.9869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0186 Acc: 0.9915: 100%|██████████| 56/56 [00:08<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0186 Acc: 0.9915\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0411 Acc: 0.9871: 100%|██████████| 258/258 [00:54<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0411 Acc: 0.9871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0170 Acc: 0.9927: 100%|██████████| 56/56 [00:08<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0170 Acc: 0.9927\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0417 Acc: 0.9861: 100%|██████████| 258/258 [00:53<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0417 Acc: 0.9861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0175 Acc: 0.9927: 100%|██████████| 56/56 [00:08<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0175 Acc: 0.9927\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0386 Acc: 0.9865: 100%|██████████| 258/258 [00:54<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0386 Acc: 0.9865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0159 Acc: 0.9938: 100%|██████████| 56/56 [00:08<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0159 Acc: 0.9938\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0454 Acc: 0.9842: 100%|██████████| 258/258 [00:53<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0454 Acc: 0.9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0166 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0166 Acc: 0.9932\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0374 Acc: 0.9853: 100%|██████████| 258/258 [00:53<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0374 Acc: 0.9853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0165 Acc: 0.9932: 100%|██████████| 56/56 [00:08<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0165 Acc: 0.9932\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0384 Acc: 0.9870: 100%|██████████| 258/258 [00:54<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0384 Acc: 0.9870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0164 Acc: 0.9938: 100%|██████████| 56/56 [00:08<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0164 Acc: 0.9938\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0406 Acc: 0.9880: 100%|██████████| 258/258 [00:54<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0406 Acc: 0.9880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0164 Acc: 0.9938: 100%|██████████| 56/56 [00:08<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0164 Acc: 0.9938\n",
      "\n",
      "Training complete in 70m 48s\n",
      "Best val Acc: 0.994353\n",
      "\n",
      "Evaluating efficientnet-b0 model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 56/56 [00:20<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      " Bacterial Leaf Blight       0.99      0.98      0.99       180\n",
      "            Brown Spot       1.00      1.00      1.00       232\n",
      "     Healthy Rice Leaf       0.99      1.00      0.99       163\n",
      "            Leaf Blast       0.98      1.00      0.99       263\n",
      "            Leaf scald       1.00      0.98      0.99       200\n",
      "Narrow Brown Leaf Spot       1.00      0.99      0.99       144\n",
      "            Neck Blast       1.00      1.00      1.00       150\n",
      "            Rice Hispa       1.00      1.00      1.00       195\n",
      "         Sheath Blight       0.99      1.00      0.99       245\n",
      "\n",
      "              accuracy                           0.99      1772\n",
      "             macro avg       0.99      0.99      0.99      1772\n",
      "          weighted avg       0.99      0.99      0.99      1772\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "===== Training completed for all models =====\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "# from torch.utils.data import DataLoader, Subset\n",
    "# from torchvision import datasets, models, transforms\n",
    "# from torchvision.models.efficientnet import MBConv\n",
    "# import time\n",
    "# import copy\n",
    "# import shutil\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # 设置随机种子确保结果可复现\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # 配置参数\n",
    "# DATA_DIR = r\"D:\\homework\\论文\\论文\\project\\dataset\\archive\\Rice_Leaf_AUG\\Rice_Leaf_AUG\"  # 原始数据集路径\n",
    "# OUTPUT_DIR = \"dataset_split\"  # 划分后的数据集保存路径\n",
    "# IMAGE_SIZE = 224\n",
    "# BATCH_SIZE = 32\n",
    "# EPOCHS = 50\n",
    "# NUM_WORKERS = 4  # 数据加载的线程数\n",
    "# SPLIT_RATIO = [0.7, 0.15, 0.15]  # 训练集、验证集、测试集比例\n",
    "\n",
    "# # 模型配置\n",
    "# MODELS = {\n",
    "#     \"efficientnet-b0\": {\n",
    "#         \"model\": models.efficientnet_b0,\n",
    "#         \"pretrained\": True,\n",
    "#         \"feature_size\": 1280  # EfficientNet-B0的特征维度\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # 检查GPU是否可用\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "# if device.type == 'cuda':\n",
    "#     print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# # 注意力模块定义\n",
    "# class ChannelAttention(nn.Module):\n",
    "#     def __init__(self, channels, reduction_ratio=16):\n",
    "#         super(ChannelAttention, self).__init__()\n",
    "#         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Conv2d(channels, channels // reduction_ratio, 1, bias=False),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(channels // reduction_ratio, channels, 1, bias=False)\n",
    "#         )\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         avg_out = self.fc(self.avg_pool(x))\n",
    "#         max_out = self.fc(self.max_pool(x))\n",
    "#         out = avg_out + max_out\n",
    "#         return x * self.sigmoid(out)\n",
    "\n",
    "# class SpatialAttention(nn.Module):\n",
    "#     def __init__(self, kernel_size=7):\n",
    "#         super(SpatialAttention, self).__init__()\n",
    "#         self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "#         max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "#         out = torch.cat([avg_out, max_out], dim=1)\n",
    "#         out = self.conv(out)\n",
    "#         return x * self.sigmoid(out)\n",
    "\n",
    "# class CBAM(nn.Module):\n",
    "#     def __init__(self, channels, reduction_ratio=16):\n",
    "#         super(CBAM, self).__init__()\n",
    "#         self.channel_attention = ChannelAttention(channels, reduction_ratio)\n",
    "#         self.spatial_attention = SpatialAttention()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.channel_attention(x)\n",
    "#         x = self.spatial_attention(x)\n",
    "#         return x\n",
    "\n",
    "# def add_cbam_to_efficientnet(model):\n",
    "#     \"\"\"为EfficientNet添加CBAM模块\"\"\"\n",
    "#     features = []\n",
    "#     for layer in model.features.children():\n",
    "#         features.append(layer)\n",
    "#         if isinstance(layer, MBConv):\n",
    "#             # 获取当前MBConv层的输出通道数\n",
    "#             out_channels = layer.out_channels\n",
    "#             # 添加CBAM模块\n",
    "#             features.append(CBAM(out_channels))\n",
    "#     # 重构features模块\n",
    "#     model.features = nn.Sequential(*features)\n",
    "#     return model\n",
    "\n",
    "# def split_dataset(data_dir, output_dir, split_ratio):\n",
    "#     \"\"\"将数据集划分为训练集、验证集和测试集\"\"\"\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "    \n",
    "#     subdirs = [f.name for f in os.scandir(data_dir) if f.is_dir()]\n",
    "    \n",
    "#     for subdir in subdirs:\n",
    "#         print(f\"Processing class: {subdir}\")\n",
    "#         class_dir = os.path.join(data_dir, subdir)\n",
    "#         files = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "        \n",
    "#         # 划分数据集\n",
    "#         train_files, test_files = train_test_split(files, test_size=split_ratio[2], random_state=42)\n",
    "#         train_files, val_files = train_test_split(train_files, test_size=split_ratio[1]/(split_ratio[0]+split_ratio[1]), random_state=42)\n",
    "        \n",
    "#         # 创建输出目录\n",
    "#         for split in [\"train\", \"val\", \"test\"]:\n",
    "#             split_dir = os.path.join(output_dir, split, subdir)\n",
    "#             os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "#         # 复制文件\n",
    "#         def copy_files(files_list, split_name):\n",
    "#             for file_name in tqdm(files_list, desc=f\"Copying {split_name} files\"):\n",
    "#                 src = os.path.join(class_dir, file_name)\n",
    "#                 dst = os.path.join(output_dir, split_name, subdir, file_name)\n",
    "#                 shutil.copy(src, dst)\n",
    "        \n",
    "#         copy_files(train_files, \"train\")\n",
    "#         copy_files(val_files, \"val\")\n",
    "#         copy_files(test_files, \"test\")\n",
    "    \n",
    "#     print(f\"Dataset split completed. Saved to {output_dir}\")\n",
    "#     return output_dir\n",
    "\n",
    "# def create_data_transforms():\n",
    "#     data_transforms = {\n",
    "#         'train': transforms.Compose([\n",
    "#             transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.RandomVerticalFlip(p=0.1),\n",
    "#             transforms.RandomRotation(30),\n",
    "#             transforms.ColorJitter(\n",
    "#                 brightness=0.3,\n",
    "#                 contrast=0.3,\n",
    "#                 saturation=0.3,\n",
    "#                 hue=0.1\n",
    "#             ),\n",
    "#             transforms.RandomGrayscale(p=0.1),\n",
    "#             transforms.RandomAffine(\n",
    "#                 degrees=0,\n",
    "#                 translate=(0.1, 0.1),\n",
    "#                 scale=(0.9, 1.1),\n",
    "#                 shear=10\n",
    "#             ),\n",
    "#             transforms.RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "#             transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#         ]),\n",
    "#         'val': transforms.Compose([\n",
    "#             transforms.Resize(IMAGE_SIZE),\n",
    "#             transforms.CenterCrop(IMAGE_SIZE),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#         ]),\n",
    "#         'test': transforms.Compose([\n",
    "#             transforms.Resize(IMAGE_SIZE),\n",
    "#             transforms.CenterCrop(IMAGE_SIZE),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#         ]),\n",
    "#     }\n",
    "#     return data_transforms\n",
    "\n",
    "# def create_data_loaders(data_dir):\n",
    "#     \"\"\"创建数据加载器\"\"\"\n",
    "#     data_transforms = create_data_transforms()\n",
    "    \n",
    "#     # 创建数据集\n",
    "#     image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "#                                               data_transforms[x])\n",
    "#                       for x in ['train', 'val', 'test']}\n",
    "    \n",
    "#     # 创建数据加载器\n",
    "#     dataloaders = {x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
    "#                                  shuffle=True if x == 'train' else False,\n",
    "#                                  num_workers=NUM_WORKERS)\n",
    "#                    for x in ['train', 'val', 'test']}\n",
    "    \n",
    "#     dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "#     class_names = image_datasets['train'].classes\n",
    "    \n",
    "#     return dataloaders, dataset_sizes, class_names\n",
    "\n",
    "# def initialize_model(model_name, num_classes, feature_extract=True):\n",
    "#     \"\"\"初始化预训练模型\"\"\"\n",
    "#     model_info = MODELS[model_name]\n",
    "#     model_ft = model_info[\"model\"](pretrained=model_info[\"pretrained\"])\n",
    "    \n",
    "#     # 添加注意力模块（仅对efficientnet）\n",
    "#     if model_name == \"efficientnet-b0\":\n",
    "#         model_ft = add_cbam_to_efficientnet(model_ft)\n",
    "    \n",
    "#     # 冻结预训练模型的参数\n",
    "#     if feature_extract:\n",
    "#         for param in model_ft.parameters():\n",
    "#             param.requires_grad = False\n",
    "    \n",
    "#     # 修改分类器\n",
    "#     if model_name == \"efficientnet-b0\":\n",
    "#         in_features = model_ft.classifier[1].in_features\n",
    "#         model_ft.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "#     return model_ft\n",
    "\n",
    "# def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, model_name=\"model\"):\n",
    "#     \"\"\"训练模型\"\"\"\n",
    "#     since = time.time()\n",
    "    \n",
    "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#     best_acc = 0.0\n",
    "    \n",
    "#     # 记录训练历史\n",
    "#     history = {\n",
    "#         'train_loss': [],\n",
    "#         'train_acc': [],\n",
    "#         'val_loss': [],\n",
    "#         'val_acc': []\n",
    "#     }\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "#         print('-' * 10)\n",
    "        \n",
    "#         # 每个epoch都有一个训练和验证阶段\n",
    "#         for phase in ['train', 'val']:\n",
    "#             if phase == 'train':\n",
    "#                 model.train()\n",
    "#             else:\n",
    "#                 model.eval()\n",
    "            \n",
    "#             running_loss = 0.0\n",
    "#             running_corrects = 0\n",
    "            \n",
    "#             # 迭代数据\n",
    "#             progress_bar = tqdm(enumerate(dataloaders[phase]), total=len(dataloaders[phase]))\n",
    "#             for i, (inputs, labels) in progress_bar:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "                \n",
    "#                 # 零梯度\n",
    "#                 optimizer.zero_grad()\n",
    "                \n",
    "#                 # 前向传播\n",
    "#                 with torch.set_grad_enabled(phase == 'train'):\n",
    "#                     outputs = model(inputs)\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "                    \n",
    "#                     # 反向传播+优化\n",
    "#                     if phase == 'train':\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()\n",
    "                \n",
    "#                 # 统计\n",
    "#                 running_loss += loss.item() * inputs.size(0)\n",
    "#                 running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "#                 # 更新进度条\n",
    "#                 progress_bar.set_description(f\"{phase} Loss: {running_loss/(i*BATCH_SIZE+inputs.size(0)):.4f} Acc: {running_corrects/(i*BATCH_SIZE+inputs.size(0)):.4f}\")\n",
    "            \n",
    "#             if phase == 'train' and scheduler:\n",
    "#                 scheduler.step()\n",
    "            \n",
    "#             epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#             epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "#             print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "#             # 记录历史\n",
    "#             history[f'{phase}_loss'].append(epoch_loss)\n",
    "#             history[f'{phase}_acc'].append(epoch_acc.item())\n",
    "            \n",
    "#             # 保存最佳模型\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#                 torch.save(model.state_dict(), f\"{model_name}_best_model.pth\")\n",
    "        \n",
    "#         print()\n",
    "    \n",
    "#     time_elapsed = time.time() - since\n",
    "#     print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "#     print(f'Best val Acc: {best_acc:4f}')\n",
    "    \n",
    "#     # 加载最佳模型权重\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "#     return model, history\n",
    "\n",
    "# def evaluate_model(model, dataloader, class_names, model_name):\n",
    "#     \"\"\"评估模型\"\"\"\n",
    "#     model.eval()\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "            \n",
    "#             outputs = model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "#     # 打印分类报告+\n",
    "#     print(f\"\\n{classification_report(all_labels, all_preds, target_names=class_names)}\")\n",
    "    \n",
    "#     # 计算混淆矩阵\n",
    "#     cm = confusion_matrix(all_labels, all_preds)\n",
    "#     plot_confusion_matrix(cm, class_names, f\"{model_name}_confusion_matrix.png\")\n",
    "    \n",
    "#     return all_preds, all_labels\n",
    "\n",
    "# def plot_confusion_matrix(cm, classes, save_path, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "#     \"\"\"绘制混淆矩阵\"\"\"\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "    \n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "    \n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i in range(cm.shape[0]):\n",
    "#         for j in range(cm.shape[1]):\n",
    "#             plt.text(j, i, format(cm[i, j], fmt),\n",
    "#                      horizontalalignment=\"center\",\n",
    "#                      color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "#     plt.savefig(save_path)\n",
    "#     plt.close()\n",
    "\n",
    "# def plot_training_curves(histories, save_path='training_curves.png'):\n",
    "#     \"\"\"绘制训练曲线\"\"\"\n",
    "#     plt.figure(figsize=(12, 10))\n",
    "    \n",
    "#     # 绘制准确率曲线\n",
    "#     plt.subplot(2, 1, 1)\n",
    "#     for model_name, history in histories.items():\n",
    "#         plt.plot(history['train_acc'], label=f'{model_name} Train')\n",
    "#         plt.plot(history['val_acc'], label=f'{model_name} Val')\n",
    "    \n",
    "#     plt.title('Model Accuracy')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.legend(loc='lower right')\n",
    "#     plt.grid(True)\n",
    "    \n",
    "#     # 绘制损失曲线\n",
    "#     plt.subplot(2, 1, 2)\n",
    "#     for model_name, history in histories.items():\n",
    "#         plt.plot(history['train_loss'], label=f'{model_name} Train')\n",
    "#         plt.plot(history['val_loss'], label=f'{model_name} Val')\n",
    "    \n",
    "#     plt.title('Model Loss')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.grid(True)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path)\n",
    "#     plt.close()\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"主函数\"\"\"\n",
    "#     # 划分数据集\n",
    "#     split_dir = split_dataset(DATA_DIR, OUTPUT_DIR, SPLIT_RATIO)\n",
    "    \n",
    "#     # 创建数据加载器\n",
    "#     dataloaders, dataset_sizes, class_names = create_data_loaders(split_dir)\n",
    "#     print(f\"Classes: {class_names}\")\n",
    "#     num_classes = len(class_names)\n",
    "    \n",
    "#     all_histories = {}\n",
    "    \n",
    "#     # 训练所有模型\n",
    "#     for model_name in MODELS.keys():\n",
    "#         print(f\"\\n===== Training {model_name} model =====\")\n",
    "        \n",
    "#         # 初始化模型\n",
    "#         model_ft = initialize_model(model_name, num_classes, feature_extract=False)\n",
    "#         model_ft = model_ft.to(device)\n",
    "        \n",
    "#         # 设置损失函数和优化器\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "#         exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "        \n",
    "#         # 训练模型\n",
    "#         model_ft, history = train_model(\n",
    "#             model_ft, dataloaders, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "#             num_epochs=EPOCHS, model_name=model_name\n",
    "#         )\n",
    "        \n",
    "#         # 保存最终模型\n",
    "#         torch.save(model_ft.state_dict(), f\"{model_name}_final_model.pth\")\n",
    "        \n",
    "#         # 评估模型\n",
    "#         print(f\"\\nEvaluating {model_name} model:\")\n",
    "#         evaluate_model(model_ft, dataloaders['test'], class_names, model_name)\n",
    "        \n",
    "#         # 保存训练历史\n",
    "#         all_histories[model_name] = history\n",
    "    \n",
    "#     # 绘制训练曲线\n",
    "#     plot_training_curves(all_histories, save_path='all_models_training_curves.png')\n",
    "    \n",
    "#     print(\"\\n===== Training completed for all models =====\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置随机种子确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 配置参数\n",
    "DATA_DIR = r\"D:\\homework\\论文\\论文\\project\\dataset\\archive\\Rice_Leaf_AUG\\Rice_Leaf_AUG\"  # 原始数据集路径\n",
    "OUTPUT_DIR = \"dataset_split\"  # 划分后的数据集保存路径\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "NUM_WORKERS = 4  # 数据加载的线程数\n",
    "SPLIT_RATIO = [0.7, 0.15, 0.15]  # 训练集、验证集、测试集比例\n",
    "\n",
    "# 模型配置\n",
    "MODELS = {\n",
    "    #     \"resnet50\": {\n",
    "    #     \"model\": models.resnet50,\n",
    "    #     \"pretrained\": True,\n",
    "    #     \"feature_size\": 2048  # ResNet50的特征维度\n",
    "    # },\n",
    "    # \"vgg16\": {\n",
    "    #     \"model\": models.vgg16,\n",
    "    #     \"pretrained\": True,\n",
    "    #     \"feature_size\": 512 * 7 * 7  # VGG16的特征维度\n",
    "    # },\n",
    "    # \"efficientnet-b0\": {\n",
    "    #     \"model\": models.efficientnet_b0,\n",
    "    #     \"pretrained\": True,\n",
    "    #     \"feature_size\": 1280  # EfficientNet-B0的特征维度\n",
    "    # },\n",
    "    \"mobilenet_v3\": {\n",
    "        \"model\": models.mobilenet_v3_large,\n",
    "        \"pretrained\": True,\n",
    "        \"feature_size\": 960  # MobileNetV3的特征维度\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "# 检查GPU是否可用\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "def split_dataset(data_dir, output_dir, split_ratio):\n",
    "    \"\"\"将数据集划分为训练集、验证集和测试集\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    subdirs = [f.name for f in os.scandir(data_dir) if f.is_dir()]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        print(f\"Processing class: {subdir}\")\n",
    "        class_dir = os.path.join(data_dir, subdir)\n",
    "        files = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "        \n",
    "        # 划分数据集\n",
    "        train_files, test_files = train_test_split(files, test_size=split_ratio[2], random_state=42)\n",
    "        train_files, val_files = train_test_split(train_files, test_size=split_ratio[1]/(split_ratio[0]+split_ratio[1]), random_state=42)\n",
    "        \n",
    "        # 创建输出目录\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            split_dir = os.path.join(output_dir, split, subdir)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        # 复制文件\n",
    "        def copy_files(files_list, split_name):\n",
    "            for file_name in tqdm(files_list, desc=f\"Copying {split_name} files\"):\n",
    "                src = os.path.join(class_dir, file_name)\n",
    "                dst = os.path.join(output_dir, split_name, subdir, file_name)\n",
    "                shutil.copy(src, dst)\n",
    "        \n",
    "        copy_files(train_files, \"train\")\n",
    "        copy_files(val_files, \"val\")\n",
    "        copy_files(test_files, \"test\")\n",
    "    \n",
    "    print(f\"Dataset split completed. Saved to {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "def create_data_transforms():\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),  # 调整裁剪范围\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(p=0.1),\n",
    "            transforms.RandomRotation(30),  # 增加旋转角度\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.3,  # 增加亮度调整范围\n",
    "                contrast=0.3,    # 增加对比度调整范围\n",
    "                saturation=0.3,  # 增加饱和度调整范围\n",
    "                hue=0.1          # 添加色调调整\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            transforms.RandomAffine(  # 仿射变换\n",
    "                degrees=0,\n",
    "                translate=(0.1, 0.1),  # 平移\n",
    "                scale=(0.9, 1.1),      # 缩放\n",
    "                shear=10               # 剪切\n",
    "            ),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.2),  # 透视变换\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # 高斯模糊\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.CenterCrop(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.CenterCrop(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms\n",
    "\n",
    "def create_data_loaders(data_dir):\n",
    "    \"\"\"创建数据加载器\"\"\"\n",
    "    data_transforms = create_data_transforms()\n",
    "    \n",
    "    # 创建数据集\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    dataloaders = {x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True if x == 'train' else False,\n",
    "                                 num_workers=NUM_WORKERS)\n",
    "                   for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    \n",
    "    return dataloaders, dataset_sizes, class_names\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract=True):\n",
    "    \"\"\"初始化预训练模型\"\"\"\n",
    "    model_info = MODELS[model_name]\n",
    "    model_ft = model_info[\"model\"](pretrained=model_info[\"pretrained\"])\n",
    "    \n",
    "    # 冻结预训练模型的参数\n",
    "    if feature_extract:\n",
    "        for param in model_ft.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # 修改分类器\n",
    "    if model_name == \"efficientnet-b0\":\n",
    "        model_ft.classifier[1] = nn.Linear(model_info[\"feature_size\"], num_classes)\n",
    "    \n",
    "    elif model_name == \"mobilenet_v3\":\n",
    "        model_ft.classifier[3] = nn.Linear(model_info[\"feature_size\"], num_classes)\n",
    "    \n",
    "    elif model_name == \"resnet50\":\n",
    "        model_ft.fc = nn.Linear(model_info[\"feature_size\"], num_classes)\n",
    "    \n",
    "    elif model_name == \"vgg16\":\n",
    "        model_ft.classifier[6] = nn.Linear(model_info[\"feature_size\"], num_classes)\n",
    "    \n",
    "    return model_ft\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, model_name=\"model\"):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # 记录训练历史\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # 每个epoch都有一个训练和验证阶段\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 训练模式\n",
    "            else:\n",
    "                model.eval()   # 评估模式\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # 迭代数据\n",
    "            progress_bar = tqdm(enumerate(dataloaders[phase]), total=len(dataloaders[phase]))\n",
    "            for i, (inputs, labels) in progress_bar:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # 零梯度\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向传播\n",
    "                # 只有在训练时才跟踪历史\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # 只有在训练阶段才进行反向传播和优化\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # 统计\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                # 更新进度条\n",
    "                progress_bar.set_description(f\"{phase} Loss: {running_loss/(i*BATCH_SIZE+inputs.size(0)):.4f} Acc: {running_corrects/(i*BATCH_SIZE+inputs.size(0)):.4f}\")\n",
    "            \n",
    "            if phase == 'train' and scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # 记录历史\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
    "            \n",
    "            # 深拷贝模型\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), f\"{model_name}_best_model.pth\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    \n",
    "    # 加载最佳模型权重\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names, model_name):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 打印分类报告\n",
    "    print(f\"\\n{classification_report(all_labels, all_preds, target_names=class_names)}\")\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plot_confusion_matrix(cm, class_names, f\"{model_name}_confusion_matrix.png\")\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, save_path, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"绘制混淆矩阵\"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_training_curves(histories, save_path='training_curves.png'):\n",
    "    \"\"\"绘制多个模型的训练曲线\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # 绘制准确率曲线\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for model_name, history in histories.items():\n",
    "        plt.plot(history['train_acc'], label=f'{model_name} Train')\n",
    "        plt.plot(history['val_acc'], label=f'{model_name} Val')\n",
    "    \n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for model_name, history in histories.items():\n",
    "        plt.plot(history['train_loss'], label=f'{model_name} Train')\n",
    "        plt.plot(history['val_loss'], label=f'{model_name} Val')\n",
    "    \n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 划分数据集\n",
    "    split_dir = split_dataset(DATA_DIR, OUTPUT_DIR, SPLIT_RATIO)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    dataloaders, dataset_sizes, class_names = create_data_loaders(split_dir)\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    all_histories = {}\n",
    "    \n",
    "    # 训练所有模型\n",
    "    for model_name in MODELS.keys():\n",
    "        print(f\"\\n===== Training {model_name} model =====\")\n",
    "        \n",
    "        # 初始化模型\n",
    "        model_ft = initialize_model(model_name, num_classes, feature_extract=False)\n",
    "        model_ft = model_ft.to(device)\n",
    "        \n",
    "        # 设置损失函数和优化器\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # 只优化最后一层的参数\n",
    "        optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "        \n",
    "        # 学习率调度器\n",
    "        exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "        \n",
    "        # 训练模型\n",
    "        model_ft, history = train_model(\n",
    "            model_ft, dataloaders, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "            num_epochs=EPOCHS, model_name=model_name\n",
    "        )\n",
    "        \n",
    "        # 保存最终模型\n",
    "        torch.save(model_ft.state_dict(), f\"{model_name}_final_model.pth\")\n",
    "        \n",
    "        # 评估模型\n",
    "        print(f\"\\nEvaluating {model_name} model:\")\n",
    "        evaluate_model(model_ft, dataloaders['test'], class_names, model_name)\n",
    "        \n",
    "        # 保存训练历史\n",
    "        all_histories[model_name] = history\n",
    "    \n",
    "    # 绘制所有模型的训练曲线\n",
    "    plot_training_curves(all_histories, save_path='all_models_training_curves.png')\n",
    "    \n",
    "    print(\"\\n===== Training completed for all models =====\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1791485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置随机种子确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 配置参数\n",
    "DATA_DIR = r\"D:\\homework\\论文\\论文\\project\\dataset\\archive\\Rice_Leaf_AUG\\Rice_Leaf_AUG\"  # 原始数据集路径\n",
    "OUTPUT_DIR = \"dataset_split\"  # 划分后的数据集保存路径\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "NUM_WORKERS = 4  # 数据加载的线程数\n",
    "SPLIT_RATIO = [0.7, 0.15, 0.15]  # 训练集、验证集、测试集比例\n",
    "\n",
    "# 模型配置\n",
    "MODELS = {\n",
    "    #     \"resnet50\": {\n",
    "    #     \"model\": models.resnet50,\n",
    "    #     \"pretrained\": True,\n",
    "    #     \"feature_size\": 2048  # ResNet50的特征维度\n",
    "    # },\n",
    "    # \"vgg16\": {\n",
    "    #     \"model\": models.vgg16,\n",
    "    #     \"pretrained\": True,\n",
    "    #     \"feature_size\": 512 * 7 * 7  # VGG16的特征维度\n",
    "    # },\n",
    "    # \"efficientnet-b0\": {\n",
    "    #     \"model\": models.efficientnet_b0,\n",
    "    #     \"pretrained\": True,\n",
    "    #     \"feature_size\": 1280  # EfficientNet-B0的特征维度\n",
    "    # },\n",
    "    \"mobilenet_v3\": {\n",
    "        \"model\": models.mobilenet_v3_large,\n",
    "        \"pretrained\": True,\n",
    "        \"feature_size\": 960  # MobileNetV3的特征维度\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "# 检查GPU是否可用\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "def split_dataset(data_dir, output_dir, split_ratio):\n",
    "    \"\"\"将数据集划分为训练集、验证集和测试集\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    subdirs = [f.name for f in os.scandir(data_dir) if f.is_dir()]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        print(f\"Processing class: {subdir}\")\n",
    "        class_dir = os.path.join(data_dir, subdir)\n",
    "        files = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "        \n",
    "        # 划分数据集\n",
    "        train_files, test_files = train_test_split(files, test_size=split_ratio[2], random_state=42)\n",
    "        train_files, val_files = train_test_split(train_files, test_size=split_ratio[1]/(split_ratio[0]+split_ratio[1]), random_state=42)\n",
    "        \n",
    "        # 创建输出目录\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            split_dir = os.path.join(output_dir, split, subdir)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        # 复制文件\n",
    "        def copy_files(files_list, split_name):\n",
    "            for file_name in tqdm(files_list, desc=f\"Copying {split_name} files\"):\n",
    "                src = os.path.join(class_dir, file_name)\n",
    "                dst = os.path.join(output_dir, split_name, subdir, file_name)\n",
    "                shutil.copy(src, dst)\n",
    "        \n",
    "        copy_files(train_files, \"train\")\n",
    "        copy_files(val_files, \"val\")\n",
    "        copy_files(test_files, \"test\")\n",
    "    \n",
    "    print(f\"Dataset split completed. Saved to {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "def create_data_transforms():\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),  # 调整裁剪范围\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(p=0.1),\n",
    "            transforms.RandomRotation(30),  # 增加旋转角度\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.3,  # 增加亮度调整范围\n",
    "                contrast=0.3,    # 增加对比度调整范围\n",
    "                saturation=0.3,  # 增加饱和度调整范围\n",
    "                hue=0.1          # 添加色调调整\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            transforms.RandomAffine(  # 仿射变换\n",
    "                degrees=0,\n",
    "                translate=(0.1, 0.1),  # 平移\n",
    "                scale=(0.9, 1.1),      # 缩放\n",
    "                shear=10               # 剪切\n",
    "            ),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.2),  # 透视变换\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # 高斯模糊\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.CenterCrop(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.CenterCrop(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms\n",
    "\n",
    "def create_data_loaders(data_dir):\n",
    "    \"\"\"创建数据加载器\"\"\"\n",
    "    data_transforms = create_data_transforms()\n",
    "    \n",
    "    # 创建数据集\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    dataloaders = {x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True if x == 'train' else False,\n",
    "                                 num_workers=NUM_WORKERS)\n",
    "                   for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    \n",
    "    return dataloaders, dataset_sizes, class_names\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract=True):\n",
    "    \"\"\"初始化预训练模型\"\"\"\n",
    "    model_info = MODELS[model_name]\n",
    "    model_ft = model_info[\"model\"](pretrained=model_info[\"pretrained\"])\n",
    "    \n",
    "    # 冻结预训练模型的参数\n",
    "    if feature_extract:\n",
    "        for param in model_ft.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # 修改分类器\n",
    "    if model_name == \"efficientnet-b0\":\n",
    "        model_ft.classifier[1] = nn.Linear(model_info[\"feature_size\"], num_classes)\n",
    "    \n",
    "    elif model_name == \"mobilenet_v3\":\n",
    "        model_ft.classifier[3] = nn.Linear(model_info[\"feature_size\"], num_classes)\n",
    "    \n",
    "    elif model_name == \"resnet50\":\n",
    "        model_ft.fc = nn.Linear(model_info[\"feature_size\"], num_classes)\n",
    "    \n",
    "    elif model_name == \"vgg16\":\n",
    "        model_ft.classifier[6] = nn.Linear(model_info[\"feature_size\"], num_classes)\n",
    "    \n",
    "    return model_ft\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, model_name=\"model\"):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # 记录训练历史\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # 每个epoch都有一个训练和验证阶段\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 训练模式\n",
    "            else:\n",
    "                model.eval()   # 评估模式\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # 迭代数据\n",
    "            progress_bar = tqdm(enumerate(dataloaders[phase]), total=len(dataloaders[phase]))\n",
    "            for i, (inputs, labels) in progress_bar:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # 零梯度\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向传播\n",
    "                # 只有在训练时才跟踪历史\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # 只有在训练阶段才进行反向传播和优化\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # 统计\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                # 更新进度条\n",
    "                progress_bar.set_description(f\"{phase} Loss: {running_loss/(i*BATCH_SIZE+inputs.size(0)):.4f} Acc: {running_corrects/(i*BATCH_SIZE+inputs.size(0)):.4f}\")\n",
    "            \n",
    "            if phase == 'train' and scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # 记录历史\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
    "            \n",
    "            # 深拷贝模型\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), f\"{model_name}_best_model.pth\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    \n",
    "    # 加载最佳模型权重\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names, model_name):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 打印分类报告\n",
    "    print(f\"\\n{classification_report(all_labels, all_preds, target_names=class_names)}\")\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plot_confusion_matrix(cm, class_names, f\"{model_name}_confusion_matrix.png\")\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, save_path, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"绘制混淆矩阵\"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_training_curves(histories, save_path='training_curves.png'):\n",
    "    \"\"\"绘制多个模型的训练曲线\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # 绘制准确率曲线\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for model_name, history in histories.items():\n",
    "        plt.plot(history['train_acc'], label=f'{model_name} Train')\n",
    "        plt.plot(history['val_acc'], label=f'{model_name} Val')\n",
    "    \n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for model_name, history in histories.items():\n",
    "        plt.plot(history['train_loss'], label=f'{model_name} Train')\n",
    "        plt.plot(history['val_loss'], label=f'{model_name} Val')\n",
    "    \n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 划分数据集\n",
    "    split_dir = split_dataset(DATA_DIR, OUTPUT_DIR, SPLIT_RATIO)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    dataloaders, dataset_sizes, class_names = create_data_loaders(split_dir)\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    all_histories = {}\n",
    "    \n",
    "    # 训练所有模型\n",
    "    for model_name in MODELS.keys():\n",
    "        print(f\"\\n===== Training {model_name} model =====\")\n",
    "        \n",
    "        # 初始化模型\n",
    "        model_ft = initialize_model(model_name, num_classes, feature_extract=False)\n",
    "        model_ft = model_ft.to(device)\n",
    "        \n",
    "        # 设置损失函数和优化器\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # 只优化最后一层的参数\n",
    "        optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "        \n",
    "        # 学习率调度器\n",
    "        exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "        \n",
    "        # 训练模型\n",
    "        model_ft, history = train_model(\n",
    "            model_ft, dataloaders, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "            num_epochs=EPOCHS, model_name=model_name\n",
    "        )\n",
    "        \n",
    "        # 保存最终模型\n",
    "        torch.save(model_ft.state_dict(), f\"{model_name}_final_model.pth\")\n",
    "        \n",
    "        # 评估模型\n",
    "        print(f\"\\nEvaluating {model_name} model:\")\n",
    "        evaluate_model(model_ft, dataloaders['test'], class_names, model_name)\n",
    "        \n",
    "        # 保存训练历史\n",
    "        all_histories[model_name] = history\n",
    "    \n",
    "    # 绘制所有模型的训练曲线\n",
    "    plot_training_curves(all_histories, save_path='all_models_training_curves.png')\n",
    "    \n",
    "    print(\"\\n===== Training completed for all models =====\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
